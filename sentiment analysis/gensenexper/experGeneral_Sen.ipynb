{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning and preprocessing of raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hotel data and input data loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Bidirectional, LSTM, Dense, Dropout\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialize dictionaries to hold the DataFrames\n",
    "hotel_dfs = {}\n",
    "input_data_dfs = {}\n",
    "\n",
    "# Load hotel_dfs from CSV files\n",
    "hotel_data_dir = '../Gen_sen/data_pre/hotel_data'\n",
    "for filename in os.listdir(hotel_data_dir):\n",
    "    if filename.endswith('_hotel_data.csv'):\n",
    "        hotel_name = filename.replace('_hotel_data.csv', '')\n",
    "        hotel_dfs[hotel_name] = pd.read_csv(os.path.join(hotel_data_dir, filename))\n",
    "\n",
    "# Load input_data_dfs from CSV files\n",
    "input_data_dir = '../Gen_sen/data_pre/input_data'\n",
    "for filename in os.listdir(input_data_dir):\n",
    "    if filename.endswith('_input_data.csv'):\n",
    "        hotel_name = filename.replace('_input_data.csv', '')\n",
    "        input_data_dfs[hotel_name] = pd.read_csv(os.path.join(input_data_dir, filename))\n",
    "\n",
    "print(\"Hotel data and input data loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Initialize combined lists\n",
    "combined_X_train = []\n",
    "combined_y_train = []\n",
    "# Initialize combined test lists\n",
    "\n",
    "raw_train_data = {}\n",
    "raw_test_data = {}\n",
    "\n",
    "\n",
    "\n",
    "for hotel_name, df in input_data_dfs.items():\n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
    "        df['cleaned_content'], \n",
    "        df['label'], \n",
    "        test_size=0.3,\n",
    "        stratify=df['label'], \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Store raw train/test data\n",
    "    raw_train_data[hotel_name] = (X_train_raw, y_train)\n",
    "    raw_test_data[hotel_name] = (X_test_raw, y_test)\n",
    "\n",
    "    # Add to combined lists\n",
    "    combined_X_train.extend(X_train_raw) \n",
    "    combined_y_train.extend(y_train.tolist())  # Ensure y_train is a list to extend it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(combined_X_train, combined_y_train, raw_test_data, num_words=5000, maxlen=100):\n",
    "    tokenizer = Tokenizer(num_words=num_words, oov_token='<OOV>')\n",
    "    tokenizer.fit_on_texts(combined_X_train)\n",
    "\n",
    "    X_train_sequences = tokenizer.texts_to_sequences(combined_X_train)\n",
    "    X_test_sequences = [tokenizer.texts_to_sequences(test_data) for _, (test_data, _) in raw_test_data.items()]\n",
    "\n",
    "    X_train_padded = pad_sequences(X_train_sequences, maxlen=maxlen)\n",
    "    X_test_padded = [pad_sequences(sequences, maxlen=maxlen) for sequences in X_test_sequences]\n",
    "\n",
    "    X_train_padded, combined_y_train = shuffle(X_train_padded, combined_y_train, random_state=42)\n",
    "\n",
    "    train_data = (X_train_padded, np.array(combined_y_train))\n",
    "    test_data = {hotel_name: (X_test_padded[i], raw_test_data[hotel_name][1]) for i, hotel_name in enumerate(raw_test_data.keys())}\n",
    "    print(len(X_train_padded), len(combined_y_train))\n",
    "    return train_data, test_data, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import ADASYN, SMOTE, RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from collections import Counter\n",
    "\n",
    "# Define a function to choose the resampling method\n",
    "def resample_data(X_train, y_train, method='SMOTE'):\n",
    "    \"\"\"\n",
    "    Resample the dataset using the chosen method.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train: Features of the training set.\n",
    "    - y_train: Labels of the training set.\n",
    "    - method: Resampling method. Choose from 'SMOTE', 'ADASYN', 'RandomUnderSampler',\n",
    "              'RandomOverSampler', 'SMOTEENN', or 'SMOTETomek'.\n",
    "    \n",
    "    Returns:\n",
    "    - Resampled X_train and y_train.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Select resampling method\n",
    "    if method == 'SMOTE':\n",
    "        resampler = SMOTE(random_state=42)\n",
    "    elif method == 'ADASYN':\n",
    "        resampler = ADASYN(random_state=42)\n",
    "    elif method == 'RandomUnderSampler':\n",
    "        resampler = RandomUnderSampler(random_state=42)\n",
    "    elif method == 'RandomOverSampler':\n",
    "        resampler = RandomOverSampler(random_state=42)\n",
    "    elif method == 'SMOTEENN':\n",
    "        resampler = SMOTEENN(random_state=42)\n",
    "    elif method == 'SMOTETomek':\n",
    "        resampler = SMOTETomek(random_state=42)\n",
    "    else:\n",
    "        print(\"Unknown resampling method. Choose 'SMOTE', 'ADASYN', 'RandomUnderSampler', 'RandomOverSampler', 'SMOTEENN', or 'SMOTETomek'. Returning unsampled data\")\n",
    "        return X_train, y_train\n",
    "    \n",
    "    # Perform resampling\n",
    "    X_resampled, y_resampled = resampler.fit_resample(X_train, y_train)\n",
    "    print(f\"Resampled dataset shape y: {Counter(y_resampled)}\")\n",
    "    print(f\"Resampled dataset shape X: {len(X_resampled)}\")\n",
    "    \n",
    "    return X_resampled, y_resampled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5603 5603\n",
      "5603 5603\n",
      "4482 4482 1121 1121\n",
      "Resampled dataset shape y: Counter({2: 3611, 1: 3611, 0: 3611})\n",
      "Resampled dataset shape X: 10833\n",
      "{0: 1.0, 1: 1.0, 2: 1.0}\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Argument `output` must have rank (ndim) `target.ndim - 1`. Received: target.shape=(None,), output.shape=(None, 35, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[342], line 77\u001b[0m\n\u001b[0;32m     75\u001b[0m stop_early \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# Training the model, callbacks=[stop_early]\u001b[39;00m\n\u001b[1;32m---> 77\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# Evaluate on test data\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# Assuming `X_test` and `y_test` are your test data\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Predator\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Predator\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\nn.py:619\u001b[0m, in \u001b[0;36msparse_categorical_crossentropy\u001b[1;34m(target, output, from_logits, axis)\u001b[0m\n\u001b[0;32m    613\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    614\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument `output` must be at least rank 1. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    615\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    616\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    617\u001b[0m     )\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(target\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(output\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]):\n\u001b[1;32m--> 619\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    620\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument `output` must have rank (ndim) `target.ndim - 1`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    621\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    622\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, output.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    623\u001b[0m     )\n\u001b[0;32m    624\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e1, e2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(target\u001b[38;5;241m.\u001b[39mshape, output\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]):\n\u001b[0;32m    625\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e1 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e2 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e1 \u001b[38;5;241m!=\u001b[39m e2:\n",
      "\u001b[1;31mValueError\u001b[0m: Argument `output` must have rank (ndim) `target.ndim - 1`. Received: target.shape=(None,), output.shape=(None, 35, 3)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import classification_report\n",
    "import kerastuner as kt\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Bidirectional, LSTM, Dense, Dropout,BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.metrics import Precision, Recall, AUC, BinaryAccuracy\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# Example data\n",
    "\n",
    "vocab_size = 2200\n",
    "embedding_dim = 75\n",
    "max_length = 75\n",
    "num_classes = 3\n",
    "\n",
    "train_data, test_data, tokenizer = prepare_data(combined_X_train, combined_y_train, raw_test_data, maxlen=max_length, num_words=vocab_size)\n",
    "\n",
    "\n",
    "# Build the CNN model\n",
    "def build_model(vocab_size, embedding_dim, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim))\n",
    "    model.add(Conv1D(filters=64, kernel_size=5, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "   \n",
    "    # model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "    # model.add(MaxPooling1D(pool_size=2))    \n",
    "    # model.add(Conv1D(filters=64, kernel_size=4, activation='relu'))\n",
    "    # model.add(MaxPooling1D(pool_size=2))\n",
    "    # model.add(Conv1D(filters=64, kernel_size=5, activation='relu'))\n",
    "    # model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Bidirectional(LSTM(64, kernel_regularizer=l2(0.01))))\n",
    "    # model.add(Bidirectional(LSTM(64, kernel_regularizer=l2(0.001))))\n",
    "    \n",
    "    model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'), )\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Instantiate and train the model\n",
    "model = build_model(vocab_size, embedding_dim, num_classes)\n",
    "\n",
    "X_train, y_train = train_data\n",
    "\n",
    "print(len(X_train), len(y_train))\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n",
    "print(len(X_train), len(y_train), len(X_val), len(y_val))\n",
    "\n",
    "X_train, y_train = resample_data(X_train, y_train, method='None')\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Calculate class weights based on the training labels\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "\n",
    "# Convert class weights to a dictionary (if needed for Keras)\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "print(class_weight_dict)\n",
    "\n",
    "# Train your model with class weights\n",
    "\n",
    "stop_early = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "# Training the model, callbacks=[stop_early]\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val), class_weight=class_weight_dict)\n",
    "\n",
    "# Evaluate on test data\n",
    "# Assuming `X_test` and `y_test` are your test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "Classification Report for All Hotels Combined:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.14      0.33      0.19       104\n",
      "     Neutral       0.27      0.36      0.30       363\n",
      "    Positive       0.87      0.76      0.81      1938\n",
      "\n",
      "    accuracy                           0.68      2405\n",
      "   macro avg       0.43      0.48      0.44      2405\n",
      "weighted avg       0.75      0.68      0.71      2405\n",
      "\n",
      "Classification report(s) saved to classification_reports/nostopcombined.txt\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Function to generate and save classification reports\n",
    "def classification_reports(model, test_data, mode='both', save_path='classification_report.txt'):\n",
    "    \"\"\"\n",
    "    Generate classification reports for individual hotels and/or a combined report, \n",
    "    and print to the console as well as save to a file.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: Trained model used to make predictions.\n",
    "    - test_data: Dictionary of test data (X_test, y_test) for each hotel.\n",
    "    - mode: 'individual', 'combined', or 'both' to control output.\n",
    "    - save_path: Path to save the classification report.\n",
    "    \"\"\"\n",
    "    all_y_true = []\n",
    "    all_y_pred = []\n",
    "    \n",
    "    with open(save_path, 'w') as f:\n",
    "        # Individual classification reports\n",
    "        if mode in ['individual', 'both']:\n",
    "            for hotel_name, (X_test, y_test) in test_data.items():\n",
    "                # Make predictions\n",
    "                y_pred = model.predict(X_test)\n",
    "                y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "                \n",
    "                # Collect true and predicted values for combined report if needed\n",
    "                if mode == 'both' or mode == 'combined':\n",
    "                    all_y_true.extend(y_test)\n",
    "                    all_y_pred.extend(y_pred_classes)\n",
    "                \n",
    "                # Generate classification report for individual hotel\n",
    "                report = classification_report(y_test, y_pred_classes, target_names=['Negative', 'Neutral', 'Positive'])\n",
    "                \n",
    "                # Print and save individual report to the file\n",
    "                print(f'Classification Report for {hotel_name}:')\n",
    "                print(report)\n",
    "                print('-' * 50)\n",
    "                \n",
    "                f.write(f'Classification Report for {hotel_name}:\\n')\n",
    "                f.write(report + '\\n')\n",
    "                f.write('-' * 50 + '\\n')\n",
    "        \n",
    "        # Combined classification report\n",
    "        if mode in ['combined', 'both']:\n",
    "            # If mode is only 'combined', collect predictions across all hotels\n",
    "            if mode == 'combined':\n",
    "                for hotel_name, (X_test, y_test) in test_data.items():\n",
    "                    y_pred = model.predict(X_test)\n",
    "                    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "                    all_y_true.extend(y_test)\n",
    "                    all_y_pred.extend(y_pred_classes)\n",
    "            \n",
    "            # Generate combined classification report\n",
    "            combined_report = classification_report(all_y_true, all_y_pred, target_names=['Negative', 'Neutral', 'Positive'])\n",
    "            \n",
    "            # Print and save combined report\n",
    "            print('Classification Report for All Hotels Combined:')\n",
    "            print(combined_report)\n",
    "            \n",
    "            f.write('Classification Report for All Hotels Combined:\\n')\n",
    "            f.write(combined_report + '\\n') \n",
    "\n",
    "    print(f'Classification report(s) saved to {save_path}')\n",
    "\n",
    "classification_reports(model, test_data, mode='combined', save_path=f'classification_reports/nostopcombined.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyttsx3\n",
    "import winsound\n",
    "\n",
    "# Sound alert using a predefined beep sound\n",
    "frequency = 1000  # Set Frequency To 1000 Hertz\n",
    "duration = 500   # Set Duration To 3000 ms == 3 seconds\n",
    "\n",
    "def say_alert(message, speed=150):\n",
    "    # Initialize the text-to-speech engine\n",
    "    engine = pyttsx3.init()\n",
    "    \n",
    "    # Beep sound alert\n",
    "    winsound.Beep(frequency, duration)\n",
    "\n",
    "    # Set properties\n",
    "    engine.setProperty('rate', speed)  # Speed (higher is faster)\n",
    "    engine.setProperty('volume', 0.75)     # Volume (0.0 to 1.0)\n",
    "\n",
    "    # Speak the message\n",
    "    engine.say(message)\n",
    "\n",
    "    # Wait for the speech to finish\n",
    "    engine.runAndWait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "say_alert(\"Model training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define your resampling methods\n",
    "# resampling_methods = ['None','SMOTE', 'ADASYN', 'RandomUnderSampler', 'RandomOverSampler', 'SMOTEENN', 'SMOTETomek']\n",
    "\n",
    "\n",
    "# models = {}\n",
    "# histories = {}\n",
    "# # Prepare data\n",
    "# train_data, test_data, tokenizer = prepare_data(combined_X_train, combined_y_train, raw_test_data)\n",
    "# # Main loop for resampling methods\n",
    "# for resampling_method in resampling_methods:\n",
    "#     try:\n",
    "#         print(f\"Training and hyperparameter tuning for {resampling_method}...\")\n",
    "#         X_train_padded, y_train = train_data\n",
    "\n",
    "#         # Split into training and validation sets\n",
    "#         X_train, X_val, y_train, y_val = train_test_split(\n",
    "#             X_train_padded, \n",
    "#             y_train, \n",
    "#             test_size=0.2, \n",
    "#             stratify=y_train, \n",
    "#             random_state=42\n",
    "#         )\n",
    "\n",
    "#         # Apply resampling\n",
    "#         X_train, y_train = resample_data(X_train, y_train, method=resampling_method)\n",
    "\n",
    "#         # Instantiate the tuner\n",
    "#         tuner = kt.Hyperband(\n",
    "#             build_model,\n",
    "#             objective='val_accuracy',\n",
    "#             max_epochs=10,\n",
    "#             factor=3,\n",
    "#             directory=f'tuners/{resampling_method}_hyperparameter_tuning',\n",
    "#             project_name='text_classification'\n",
    "#         )\n",
    "\n",
    "#         stop_early = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "#         # Hyperparameter search\n",
    "#         tuner.search(X_train, y_train, epochs=10, validation_data=(X_val, y_val), callbacks=[stop_early])\n",
    "\n",
    "#         # Retrieve and save best hyperparameters\n",
    "#         best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "#         with open(f'hyperparameter_tuning/{resampling_method}_best_hyperparameters.pkl', 'wb') as f:\n",
    "#             pickle.dump(best_hps.values, f)\n",
    "\n",
    "#         # Build and train the best model\n",
    "#         model = tuner.hypermodel.build(best_hps)\n",
    "#         history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, callbacks=[stop_early])\n",
    "\n",
    "#         # Save the training history\n",
    "#         history_df = pd.DataFrame(history.history)\n",
    "#         history_df.to_csv(f'hyperparameter_tuning/{resampling_method}_training_history.csv', index=False)\n",
    "        \n",
    "#         models[resampling_method] = model\n",
    "#         histories[resampling_method] = history\n",
    "\n",
    "#         print(f\"Training and hyperparameter tuning completed successfully for {resampling_method}.\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"An error occurred with {resampling_method}: {e}\")\n",
    "#         say_alert(f\"An error occurred with {resampling_method}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# # Function to evaluate the model on test data for multiple hotels\n",
    "# def evaluate_model(model, test_data):\n",
    "#     results = {}\n",
    "#     for hotel_name, (X_test, y_test) in test_data.items():\n",
    "#         loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "#         results[hotel_name] = {'accuracy': accuracy, 'loss': loss}\n",
    "#         print(f'Test Accuracy for {hotel_name}: {accuracy:.4f}')\n",
    "#         print(f'Test Loss for {hotel_name}: {loss:.4f}')\n",
    "#     return results\n",
    "\n",
    "# # Function to plot training & validation accuracy and loss values\n",
    "# def plot_training_history(history):\n",
    "#     plt.figure(figsize=(12, 4))\n",
    "\n",
    "#     # Plotting accuracy\n",
    "#     plt.subplot(1, 2, 1)\n",
    "#     plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "#     plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "#     plt.title('Model Accuracy Over Epochs')\n",
    "#     plt.ylabel('Accuracy')\n",
    "#     plt.xlabel('Epoch')\n",
    "#     plt.legend(loc='upper left')\n",
    "\n",
    "#     # Plotting loss\n",
    "#     plt.subplot(1, 2, 2)\n",
    "#     plt.plot(history.history['loss'], label='Train Loss')\n",
    "#     plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "#     plt.title('Model Loss Over Epochs')\n",
    "#     plt.ylabel('Loss')\n",
    "#     plt.xlabel('Epoch')\n",
    "#     plt.legend(loc='upper left')\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "    \n",
    "# all_results = {}\n",
    "\n",
    "# for resampling_method, model in models.items():\n",
    "#     # Evaluate model\n",
    "#     results = evaluate_model(model, test_data)\n",
    "#     all_results[resampling_method] = results\n",
    "\n",
    "#     # Plot training history\n",
    "#     print(f\"Training history for {resampling_method}:\")\n",
    "#     plot_training_history(histories[resampling_method])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# # Function to plot accuracy and loss for each hotel\n",
    "# def plot_accuracy_and_loss(results):\n",
    "#     # Extract hotel names, accuracies, and losses from the results dictionary\n",
    "#     hotels = list(results.keys())\n",
    "#     accuracy = [results[hotel]['accuracy'] for hotel in hotels]\n",
    "#     loss = [results[hotel]['loss'] for hotel in hotels]\n",
    "\n",
    "#     # Create an array for the x-axis labels\n",
    "#     x = np.arange(len(hotels))  # the label locations\n",
    "\n",
    "#     # Set up the figure\n",
    "#     plt.figure(figsize=(14, 6))\n",
    "\n",
    "#     # Plotting Accuracy\n",
    "#     plt.subplot(1, 2, 1)\n",
    "#     plt.bar(x - 0.2, accuracy, 0.4, label='Accuracy', color='skyblue')\n",
    "#     plt.xticks(x, hotels, rotation=45)\n",
    "#     plt.ylim(0, 1)  # Set y-axis limits for accuracy\n",
    "#     plt.xlabel('Hotels')\n",
    "#     plt.ylabel('Accuracy')\n",
    "#     plt.title('Test Accuracy Across Hotels')\n",
    "#     plt.legend()\n",
    "\n",
    "#     # Plotting Loss\n",
    "#     plt.subplot(1, 2, 2)\n",
    "#     plt.bar(x + 0.2, loss, 0.4, label='Loss', color='salmon')\n",
    "#     plt.xticks(x, hotels, rotation=45)\n",
    "#     plt.ylim(0, max(loss) + 0.1)  # Adjust the y-axis limit for loss\n",
    "#     plt.xlabel('Hotels')\n",
    "#     plt.ylabel('Loss')\n",
    "#     plt.title('Test Loss Across Hotels')\n",
    "#     plt.legend()\n",
    "\n",
    "#     # Adjust layout for better readability\n",
    "#     plt.tight_layout()\n",
    "\n",
    "#     # Show the plot\n",
    "#     plt.show()\n",
    "    \n",
    "\n",
    "# for resampling_method, results in all_results.items():\n",
    "#     print(f\"Results for {resampling_method}:\")\n",
    "#     plot_accuracy_and_loss(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import confusion_matrix\n",
    "# import seaborn as sns\n",
    "# from imblearn.under_sampling import RandomUnderSampler\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# from collections import Counter\n",
    "\n",
    "# # Function to downsample test data\n",
    "# def downsample_test_data(test_data):\n",
    "#     downsampled_test_data = {}\n",
    "#     for hotel_name, (X_test, y_test) in test_data.items():\n",
    "#         rus = RandomUnderSampler(random_state=42)\n",
    "#         X_test_resampled, y_test_resampled = rus.fit_resample(X_test, y_test)\n",
    "#         downsampled_test_data[hotel_name] = (X_test_resampled, y_test_resampled)\n",
    "#     return downsampled_test_data\n",
    "\n",
    "\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# from collections import Counter\n",
    "\n",
    "# # Function to generate and plot confusion matrices\n",
    "# def plot_confusion_matrices(model, test_data, mode='both', downsample=False, resampling_method='None'):\n",
    "#     \"\"\"\n",
    "#     Plot confusion matrices for each hotel or combined.\n",
    "    \n",
    "#     Parameters:\n",
    "#     - model: Trained model used to make predictions.\n",
    "#     - test_data: Dictionary of test data (X_test, y_test) for each hotel.\n",
    "#     - mode: 'individual', 'combined', or 'both' to control output.\n",
    "#     - downsample: Whether to downsample the test data for class balancing.\n",
    "#     \"\"\"\n",
    "#     if downsample:\n",
    "#         test_data = downsample_test_data(test_data)\n",
    "\n",
    "#     all_y_true = []\n",
    "#     all_y_pred = []\n",
    "\n",
    "#     # Individual confusion matrices\n",
    "#     if mode in ['individual', 'both']:\n",
    "#         for hotel_name, (X_test, y_test) in test_data.items():\n",
    "#             y_pred = model.predict(X_test)\n",
    "#             y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "#             all_y_true.extend(y_test)\n",
    "#             all_y_pred.extend(y_pred_classes)\n",
    "            \n",
    "#             label_counts = Counter(y_test)\n",
    "#             print(f\"Label distribution for {hotel_name}: {label_counts}\")\n",
    "\n",
    "#             # Generate confusion matrix for individual hotel\n",
    "#             cm = confusion_matrix(y_test, y_pred_classes)\n",
    "#             plt.figure(figsize=(10, 7))\n",
    "#             sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "#                         xticklabels=['Negative', 'Neutral', 'Positive'], \n",
    "#                         yticklabels=['Negative', 'Neutral', 'Positive'])\n",
    "#             plt.xlabel('Predicted')\n",
    "#             plt.ylabel('True')\n",
    "#             plt.title(f'Confusion Matrix for {hotel_name} (Resampling Method: {resampling_method})')\n",
    "#             plt.show()\n",
    "\n",
    "#     # Combined confusion matrix\n",
    "#     if mode in ['combined', 'both']:\n",
    "#         if mode == 'combined':  # If only combined, we need to collect y_true and y_pred\n",
    "#             for hotel_name, (X_test, y_test) in test_data.items():\n",
    "#                 y_pred = model.predict(X_test)\n",
    "#                 y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "#                 all_y_true.extend(y_test)\n",
    "#                 all_y_pred.extend(y_pred_classes)\n",
    "\n",
    "#         cm_combined = confusion_matrix(all_y_true, all_y_pred)\n",
    "#         plt.figure(figsize=(10, 7))\n",
    "#         sns.heatmap(cm_combined, annot=True, fmt='d', cmap='Blues', \n",
    "#                     xticklabels=['Negative', 'Neutral', 'Positive'], \n",
    "#                     yticklabels=['Negative', 'Neutral', 'Positive'])\n",
    "#         plt.xlabel('Predicted')\n",
    "#         plt.ylabel('True')\n",
    "#         plt.title('Confusion Matrix for All Hotels Combined (Resampling Method: {})'.format(resampling_method))\n",
    "#         plt.show()\n",
    "\n",
    "# train_data, test_data, tokenizer = prepare_data(combined_X_train, combined_y_train, raw_test_data)\n",
    "    \n",
    "# for resampling_method, model in models.items():\n",
    "#     print(f\"Confusion matrices for {resampling_method}:\")\n",
    "#     plot_confusion_matrices(model, test_data, 'combined')  # Or use downsampled_test_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import classification_report\n",
    "# import numpy as np\n",
    "\n",
    "# # Function to generate and save classification reports\n",
    "# def classification_reports(model, test_data, mode='both', save_path='classification_report.txt'):\n",
    "#     \"\"\"\n",
    "#     Generate classification reports for individual hotels and/or a combined report, \n",
    "#     and print to the console as well as save to a file.\n",
    "    \n",
    "#     Parameters:\n",
    "#     - model: Trained model used to make predictions.\n",
    "#     - test_data: Dictionary of test data (X_test, y_test) for each hotel.\n",
    "#     - mode: 'individual', 'combined', or 'both' to control output.\n",
    "#     - save_path: Path to save the classification report.\n",
    "#     \"\"\"\n",
    "#     all_y_true = []\n",
    "#     all_y_pred = []\n",
    "    \n",
    "#     with open(save_path, 'w') as f:\n",
    "#         # Individual classification reports\n",
    "#         if mode in ['individual', 'both']:\n",
    "#             for hotel_name, (X_test, y_test) in test_data.items():\n",
    "#                 # Make predictions\n",
    "#                 y_pred = model.predict(X_test)\n",
    "#                 y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "                \n",
    "#                 # Collect true and predicted values for combined report if needed\n",
    "#                 if mode == 'both' or mode == 'combined':\n",
    "#                     all_y_true.extend(y_test)\n",
    "#                     all_y_pred.extend(y_pred_classes)\n",
    "                \n",
    "#                 # Generate classification report for individual hotel\n",
    "#                 report = classification_report(y_test, y_pred_classes, target_names=['Negative', 'Neutral', 'Positive'])\n",
    "                \n",
    "#                 # Print and save individual report to the file\n",
    "#                 print(f'Classification Report for {hotel_name}:')\n",
    "#                 print(report)\n",
    "#                 print('-' * 50)\n",
    "                \n",
    "#                 f.write(f'Classification Report for {hotel_name}:\\n')\n",
    "#                 f.write(report + '\\n')\n",
    "#                 f.write('-' * 50 + '\\n')\n",
    "        \n",
    "#         # Combined classification report\n",
    "#         if mode in ['combined', 'both']:\n",
    "#             # If mode is only 'combined', collect predictions across all hotels\n",
    "#             if mode == 'combined':\n",
    "#                 for hotel_name, (X_test, y_test) in test_data.items():\n",
    "#                     y_pred = model.predict(X_test)\n",
    "#                     y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "#                     all_y_true.extend(y_test)\n",
    "#                     all_y_pred.extend(y_pred_classes)\n",
    "            \n",
    "#             # Generate combined classification report\n",
    "#             combined_report = classification_report(all_y_true, all_y_pred, target_names=['Negative', 'Neutral', 'Positive'])\n",
    "            \n",
    "#             # Print and save combined report\n",
    "#             print('Classification Report for All Hotels Combined:')\n",
    "#             print(combined_report)\n",
    "            \n",
    "#             f.write('Classification Report for All Hotels Combined:\\n')\n",
    "#             f.write(combined_report + '\\n')\n",
    "\n",
    "#     print(f'Classification report(s) saved to {save_path}')\n",
    "\n",
    "# for resampling_method, model in models.items():\n",
    "#     print(f\"Classification reports for {resampling_method}:\")\n",
    "#     classification_reports(model, test_data, mode='combined', save_path=f'classification_reports/{resampling_method}_para_tune_combined.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Count the number of cleaned reviews per hotel\n",
    "# cleaned_reviews_count = {hotel: df['cleaned_content'].notnull().sum() for hotel, df in hotel_dfs.items()}\n",
    "\n",
    "# # Print the counts\n",
    "# for hotel, count in cleaned_reviews_count.items():\n",
    "#     print(f\"{hotel}: {count} cleaned reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# output_dir = 'predictions_by_hotel'\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# for hotel_name, (X_test, y_test) in test_data.items():\n",
    "#     # Evaluate the model\n",
    "#     loss, accuracy = model.evaluate(X_test, y_test)\n",
    "#     print(f'Test Accuracy for {hotel_name}: {accuracy:.4f}')\n",
    "    \n",
    "#     # Make predictions\n",
    "#     predicted_scores = model.predict(X_test)  # Predicted scores give probabilities for each class\n",
    "    \n",
    "    \n",
    "    \n",
    "#     # Convert predicted probabilities to class labels\n",
    "#     predicted_classes = np.argmax(predicted_scores, axis=1)  # Get index of max probability for each sample\n",
    "#     # Create a DataFrame to store true labels and predicted labels\n",
    "#     predictions = {\n",
    "#         'true_label': y_test,\n",
    "#         'predicted_class': predicted_classes,\n",
    "#         'predicted_scores': predicted_scores.tolist()  # Store the original probabilities if needed\n",
    "#     }\n",
    "    \n",
    "#     # Convert to DataFrame\n",
    "#     predictions_df = pd.DataFrame(predictions)\n",
    "    \n",
    "#     # Add hotel name to the DataFrame\n",
    "#     predictions_df['hotel_name'] = hotel_name\n",
    "    \n",
    "#     # Save predictions to a CSV file, named after the hotel\n",
    "#     predictions_filename = os.path.join(output_dir, f'{hotel_name}_predictions.csv')\n",
    "#     predictions_df.to_csv(predictions_filename, index=False)\n",
    "\n",
    "#     print(f'Predictions for {hotel_name} saved to {predictions_filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import datetime\n",
    "# import os\n",
    "\n",
    "# # Assuming `model` is your Keras model\n",
    "# timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "# file_name = f'saved_models/General_Sen_{timestamp}.keras'\n",
    "\n",
    "# # Make sure the directory exists\n",
    "# os.makedirs(os.path.dirname(file_name), exist_ok=True)\n",
    "\n",
    "# # Save the model with the unique file name\n",
    "# model.save(file_name)\n",
    "# print(f\"Model saved as {file_name}\")\n",
    "# model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
