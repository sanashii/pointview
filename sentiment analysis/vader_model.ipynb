{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imported Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Predator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string, nltk, os, glob, pickle\n",
    "from bs4 import BeautifulSoup\n",
    "import re, datetime,gensim,spacy\n",
    "import pandas as pd, numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt \n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords, words \n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mvader_lexicon\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('vader_lexicon')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93msentiment/vader_lexicon.zip/vader_lexicon/vader_lexicon.txt\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Predator/nltk_data'\n    - 'c:\\\\Users\\\\Predator\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\nltk_data'\n    - 'c:\\\\Users\\\\Predator\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\Predator\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Predator\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m BAD_SYMBOLS_RE \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mcompile(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[^0-9a-z #+_]\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m STOPWORDS \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m----> 5\u001b[0m sid \u001b[38;5;241m=\u001b[39m \u001b[43mSentimentIntensityAnalyzer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# for VADER SA\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Predator\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\sentiment\\vader.py:340\u001b[0m, in \u001b[0;36mSentimentIntensityAnalyzer.__init__\u001b[1;34m(self, lexicon_file)\u001b[0m\n\u001b[0;32m    336\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    337\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    338\u001b[0m     lexicon_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentiment/vader_lexicon.zip/vader_lexicon/vader_lexicon.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    339\u001b[0m ):\n\u001b[1;32m--> 340\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlexicon_file \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlexicon_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    341\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlexicon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_lex_dict()\n\u001b[0;32m    342\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconstants \u001b[38;5;241m=\u001b[39m VaderConstants()\n",
      "File \u001b[1;32mc:\\Users\\Predator\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\data.py:750\u001b[0m, in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[1;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32mc:\\Users\\Predator\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[0;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[0;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[1;32mc:\\Users\\Predator\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mvader_lexicon\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('vader_lexicon')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93msentiment/vader_lexicon.zip/vader_lexicon/vader_lexicon.txt\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Predator/nltk_data'\n    - 'c:\\\\Users\\\\Predator\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\nltk_data'\n    - 'c:\\\\Users\\\\Predator\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\Predator\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Predator\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "words_english = words.words()\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "sid = SentimentIntensityAnalyzer() # for VADER SA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define Functions #######\n",
    "# def make_bigrams(texts):\n",
    "# \treturn [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def drop_columns(df): #! APPLICABLE FOR TRIPADVISOR REVIEWS\n",
    "    df.drop_duplicates(subset=['Author', 'Review Text'], inplace=True) # to ensure the uniqueness of reviews by author and text\n",
    "    # Drop columns 'B' and 'C'\n",
    "    columns_to_drop = ['Author', 'Review Date', 'Title', 'Date of Stay', 'Trip Type']\n",
    "    df = df.drop(columns=columns_to_drop)\n",
    "    return df\n",
    "\n",
    "def clean_ratings(df):#! APPLICABLE FOR TRIPADVISOR REVIEWS ATM\n",
    "    # Remove characters other than digits and '.0'\n",
    "    df['Rating'] = df['Rating'].str.extract(r'(\\d+(?:\\.\\d+)?)')  # Extract digits and decimal point\n",
    "    # Convert the cleaned ratings column to integer type\n",
    "    df['Rating'] = df['Rating'].astype(float).astype(int)  # Convert to float first to handle '.0' cases\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    text = BeautifulSoup(text, \"lxml\").text # HTML decoding\n",
    "    text = text.lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwords from text\n",
    "    return text\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    '''\n",
    "    Utility function to classify the polarity of a text\n",
    "    using VADER.\n",
    "    '''\n",
    "    # Analyze sentiment using VADER\n",
    "    scores = sid.polarity_scores(text)\n",
    "    \n",
    "    return scores['compound']\n",
    "    # # Classify the sentiment\n",
    "    # if scores['compound'] >= 0.05:\n",
    "    #     return 1  # Positive\n",
    "    # elif scores['compound'] <= -0.05:\n",
    "    #     return -1  # Negative\n",
    "    # else:\n",
    "    #     return 0  # Neutral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading of Data + Cleaning + Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from the specified CSV files\n",
    "waterfront_reviews = pd.read_csv(r'..\\datasets\\tripadvisor\\1_Waterfront-Cebu-City-Hotel-Casino.csv')\n",
    "bai_reviews = pd.read_csv(r'..\\datasets\\tripadvisor\\3_bai-Hotel-Cebu.csv')\n",
    "\n",
    "# Display the first few rows of each DataFrame\n",
    "print(\"Waterfront Reviews:\")\n",
    "waterfront_reviews = drop_columns(waterfront_reviews) # should drop columns unnecessary for sentiment analysis\n",
    "waterfront_reviews = clean_ratings(waterfront_reviews) # should clean the ratings column\n",
    "# waterfront_reviews.shape # (1725, 2)\n",
    "# (waterfront_reviews.isnull().sum()/(len(waterfront_reviews)))*100 # no missing values\n",
    "waterfront_reviews.name = 'Waterfront Hotel and Casino'\n",
    "waterfront_reviews.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nBai Hotel Reviews:\")\n",
    "bai_reviews = drop_columns(bai_reviews)\n",
    "bai_reviews = clean_ratings(bai_reviews)\n",
    "# bai_reviews.shape # (726, 2)\n",
    "# (bai_reviews.isnull().sum()/(len(bai_reviews)))*100 # no missing values\n",
    "bai_reviews.name = 'bai Hotel'\n",
    "bai_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process data for the specified hotels\n",
    "hotel_dataframes = [waterfront_reviews, bai_reviews]\n",
    "\n",
    "for hotel_df in hotel_dataframes:\n",
    "    # Extract the hotel name from the DataFrame variable name\n",
    "    hotel_name = hotel_df.name\n",
    "    \n",
    "    print('Analyzing', hotel_name)\n",
    "    \n",
    "    # Drop rows with NaN values in 'Review Text' column\n",
    "    hotel_df.dropna(subset=['Review Text'], inplace=True)\n",
    "    \n",
    "    # Apply text cleaning\n",
    "    hotel_df['Review Text'] = hotel_df['Review Text'].apply(clean_text)\n",
    "    \n",
    "    # Perform sentiment analysis and add the results to the DataFrame\n",
    "    hotel_df['SA'] = hotel_df['Review Text'].apply(analyze_sentiment)\n",
    "    \n",
    "    # Calculate percentages of positive, neutral, and negative reviews\n",
    "    pos_texts = int(len(hotel_df[hotel_df['SA'] > 0]))\n",
    "    neu_texts = int(len(hotel_df[hotel_df['SA'] == 0]))\n",
    "    neg_texts = int(len(hotel_df[hotel_df['SA'] < 0]))\n",
    "\n",
    "    print(\"Percentage of Positive Reviews: {}%\".format(pos_texts * 100 / len(hotel_df)))\n",
    "    print(\"Percentage of Neutral Reviews: {}%\".format(neu_texts * 100 / len(hotel_df)))\n",
    "    print(\"Percentage of Negative Reviews: {}%\".format(neg_texts * 100 / len(hotel_df)))\n",
    "    \n",
    "    # Save sentiment analysis results to a CSV file\n",
    "    sentiment_count = pd.DataFrame({\n",
    "        '%Positive': [pos_texts * 100 / len(hotel_df)],\n",
    "        '%Neutral': [neu_texts * 100 / len(hotel_df)],\n",
    "        '%Negative': [neg_texts * 100 / len(hotel_df)]\n",
    "    })\n",
    "    output_folder = r'sentiment count'\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    output_filename = os.path.join(output_folder, hotel_name.replace(' ', '_') + '_sentiment_count.csv')\n",
    "    sentiment_count.to_csv(output_filename, index=False)\n",
    "    \n",
    "    # Categorize sentiments\n",
    "    condition = [hotel_df['SA'] < 0, hotel_df['SA'] == 0, hotel_df['SA'] > 0]\n",
    "    choices = ['Negative', 'Neutral', 'Positive']\n",
    "    hotel_df['Sentiment'] = np.select(condition, choices)\n",
    "    \n",
    "    # Save processed data\n",
    "    output_folder = r'processed reviews'\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    output_filename = os.path.join(output_folder, hotel_name.replace(' ', '_') + '_processed.csv')\n",
    "    hotel_df.to_csv(output_filename, index=False)\n",
    "    print(\"Processed data saved to:\", output_filename)\n",
    "    print('\\n')\n",
    "    \n",
    "    # to add more analysis or processing steps when necessary\n",
    "    # ...\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
