{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPERIMENT #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\andyb\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\andyb\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n",
      "Model built successfully\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFBertModel\n",
    "\n",
    "# Test loading a simple model\n",
    "try:\n",
    "    bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "    print(\"Model loaded successfully\")\n",
    "    \n",
    "    # Explicitly build the model to ensure it doesn't have issues\n",
    "    dummy_input_ids = tf.keras.Input(shape=(200,), dtype=tf.int32)\n",
    "    dummy_attention_mask = tf.keras.Input(shape=(200,), dtype=tf.int32)\n",
    "    _ = bert_model([dummy_input_ids, dummy_attention_mask])\n",
    "\n",
    "    print(\"Model built successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 29 CSV files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andyb\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer 'tf_bert_model_1' (type TFBertModel).\n\nData of type <class 'keras.src.backend.common.keras_tensor.KerasTensor'> is not allowed only (<class 'tensorflow.python.framework.tensor.Tensor'>, <class 'bool'>, <class 'int'>, <class 'transformers.utils.generic.ModelOutput'>, <class 'tuple'>, <class 'list'>, <class 'dict'>, <class 'numpy.ndarray'>) is accepted for attention_mask.\n\nCall arguments received by layer 'tf_bert_model_1' (type TFBertModel):\n  • input_ids=<KerasTensor shape=(None, 200), dtype=int32, sparse=False, name=input_ids>\n  • attention_mask=<KerasTensor shape=(None, 200), dtype=int32, sparse=False, name=attention_mask>\n  • token_type_ids=None\n  • position_ids=None\n  • head_mask=None\n  • inputs_embeds=None\n  • encoder_hidden_states=None\n  • encoder_attention_mask=None\n  • past_key_values=None\n  • use_cache=None\n  • output_attentions=None\n  • output_hidden_states=None\n  • return_dict=None\n  • training=False",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 86\u001b[0m\n\u001b[0;32m     83\u001b[0m attention_masks \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mInput(shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m200\u001b[39m,), dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mint32, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# Pass input_ids and attention_masks as separate arguments\u001b[39;00m\n\u001b[1;32m---> 86\u001b[0m bert_output \u001b[38;5;241m=\u001b[39m \u001b[43mbert_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_masks\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# [1] is the pooled output\u001b[39;00m\n\u001b[0;32m     87\u001b[0m dense \u001b[38;5;241m=\u001b[39m Dense(\u001b[38;5;241m64\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m)(bert_output)\n\u001b[0;32m     88\u001b[0m dropout \u001b[38;5;241m=\u001b[39m Dropout(\u001b[38;5;241m0.3\u001b[39m)(dense)\n",
      "File \u001b[1;32mc:\\Users\\andyb\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\andyb\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\modeling_tf_utils.py:436\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    434\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m--> 436\u001b[0m unpacked_inputs \u001b[38;5;241m=\u001b[39m \u001b[43minput_processing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args_and_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    437\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munpacked_inputs)\n",
      "File \u001b[1;32mc:\\Users\\andyb\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\modeling_tf_utils.py:513\u001b[0m, in \u001b[0;36minput_processing\u001b[1;34m(func, config, **kwargs)\u001b[0m\n\u001b[0;32m    511\u001b[0m         output[k] \u001b[38;5;241m=\u001b[39m v\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 513\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(v)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not allowed only \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mallowed_types\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is accepted for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(main_input, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, \u001b[38;5;28minput\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(main_input):\n\u001b[0;32m    517\u001b[0m         \u001b[38;5;66;03m# EagerTensors don't allow to use the .name property so we check for a real Tensor\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling layer 'tf_bert_model_1' (type TFBertModel).\n\nData of type <class 'keras.src.backend.common.keras_tensor.KerasTensor'> is not allowed only (<class 'tensorflow.python.framework.tensor.Tensor'>, <class 'bool'>, <class 'int'>, <class 'transformers.utils.generic.ModelOutput'>, <class 'tuple'>, <class 'list'>, <class 'dict'>, <class 'numpy.ndarray'>) is accepted for attention_mask.\n\nCall arguments received by layer 'tf_bert_model_1' (type TFBertModel):\n  • input_ids=<KerasTensor shape=(None, 200), dtype=int32, sparse=False, name=input_ids>\n  • attention_mask=<KerasTensor shape=(None, 200), dtype=int32, sparse=False, name=attention_mask>\n  • token_type_ids=None\n  • position_ids=None\n  • head_mask=None\n  • inputs_embeds=None\n  • encoder_hidden_states=None\n  • encoder_attention_mask=None\n  • past_key_values=None\n  • use_cache=None\n  • output_attentions=None\n  • output_hidden_states=None\n  • return_dict=None\n  • training=False"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "# Load and concatenate all CSV files into a single DataFrame\n",
    "def load_aspect_data(path):\n",
    "    all_files = glob.glob(path)\n",
    "    all_data = []\n",
    "    for filename in all_files:\n",
    "        df = pd.read_csv(filename)\n",
    "        all_data.append(df)\n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "    print(f\"Loaded {len(all_files)} CSV files.\")\n",
    "    return combined_df\n",
    "\n",
    "aspect_path = r\"C:\\Users\\andyb\\Desktop\\Coding Files\\PointView\\datasets\\aspect_based_dataset\\*.csv\"\n",
    "aspect_df = load_aspect_data(aspect_path)\n",
    "\n",
    "# Ensure 'Classification' is categorical and has correct unique values\n",
    "specific_kpis = ['food', 'staff', 'comfort & facilities', 'value for money']\n",
    "\n",
    "# Binary encoding for each KPI in 'Classification' column\n",
    "def check_kpi(classification, kpi):\n",
    "    if isinstance(classification, str):\n",
    "        return 1 if kpi.lower() in classification.lower() else 0\n",
    "    return 0\n",
    "\n",
    "for kpi in specific_kpis:\n",
    "    aspect_df[kpi] = aspect_df['Classification'].apply(lambda x: check_kpi(x, kpi))\n",
    "\n",
    "# Prepare features (X) and labels (y)\n",
    "X = aspect_df['Opinion'].values\n",
    "y = aspect_df[specific_kpis].values\n",
    "\n",
    "# Load pre-trained BERT tokenizer and model\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to tokenize and encode data\n",
    "def bert_encode(texts, tokenizer, max_len=200):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for text in texts:\n",
    "        encoded = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='tf'\n",
    "        )\n",
    "        input_ids.append(encoded['input_ids'])\n",
    "        attention_masks.append(encoded['attention_mask'])\n",
    "\n",
    "    return {\n",
    "        'input_ids': tf.concat(input_ids, axis=0),\n",
    "        'attention_mask': tf.concat(attention_masks, axis=0)\n",
    "    }\n",
    "\n",
    "X_encoded = bert_encode(X, bert_tokenizer)\n",
    "\n",
    "# Convert TensorFlow tensors to NumPy arrays before splitting\n",
    "X_input_ids = X_encoded['input_ids'].numpy()\n",
    "X_attention_masks = X_encoded['attention_mask'].numpy()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_ids, X_test_ids, y_train, y_test = train_test_split(X_input_ids, y, test_size=0.2, random_state=42)\n",
    "attention_train, attention_test = train_test_split(X_attention_masks, test_size=0.2, random_state=42)\n",
    "\n",
    "# Load BERT model\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Build the BERT-based model\n",
    "input_ids = tf.keras.Input(shape=(200,), dtype=tf.int32, name='input_ids')\n",
    "attention_masks = tf.keras.Input(shape=(200,), dtype=tf.int32, name='attention_mask')\n",
    "\n",
    "# Pass input_ids and attention_masks as separate arguments\n",
    "bert_output = bert_model(input_ids=input_ids, attention_mask=attention_masks)[1]  # [1] is the pooled output\n",
    "dense = Dense(64, activation='relu')(bert_output)\n",
    "dropout = Dropout(0.3)(dense)\n",
    "output = Dense(len(specific_kpis), activation='sigmoid')(dropout)\n",
    "\n",
    "model = tf.keras.Model(inputs=[input_ids, attention_masks], outputs=output)\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6, verbose=1)\n",
    "\n",
    "# Train the BERT model\n",
    "history = model.fit(\n",
    "    [X_train_ids, attention_train],\n",
    "    y_train,\n",
    "    batch_size=16,\n",
    "    epochs=10,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stopping, lr_reducer]\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate([X_test_ids, attention_test], y_test)\n",
    "print(f\"Test accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Detailed classification report\n",
    "y_pred = model.predict([X_test_ids, attention_test])\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "print(classification_report(y_test, y_pred_binary, target_names=specific_kpis))\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss Over Epochs')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Accuracy Over Epochs')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Function to predict KPI likelihoods for new data using BERT\n",
    "def predict_kpi_likelihoods(text):\n",
    "    encoded_text = bert_encode([text], bert_tokenizer)\n",
    "    prediction = model.predict([encoded_text['input_ids'].numpy(), encoded_text['attention_mask'].numpy()])[0]\n",
    "    return {kpi: float(likelihood) for kpi, likelihood in zip(specific_kpis, prediction)}\n",
    "\n",
    "# Test the model on a sample review\n",
    "sample_review = \"The room was clean and comfortable, but the staff was not very friendly. The food was excellent.\"\n",
    "predicted_likelihoods = predict_kpi_likelihoods(sample_review)\n",
    "print(\"\\nPredicted KPI likelihoods for the sample review:\")\n",
    "for kpi, likelihood in predicted_likelihoods.items():\n",
    "    print(f\"{kpi}: {likelihood:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPERIMENT #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Opinion  \\\n",
      "0  We stayed for a week and could not fault it at...   \n",
      "1  This resort is beautiful. The rooms are fabulo...   \n",
      "2  i never fail to visit Shangrila Boracay eveyti...   \n",
      "3  This is really a 4.5 star review. Had a chance...   \n",
      "4  Transfers - On arrival at the airport we were ...   \n",
      "\n",
      "                       Classification  \n",
      "0                      Staff:Location  \n",
      "1           Food:Comfort & Facilities  \n",
      "2                       Location:Food  \n",
      "3  Comfort & Facilities:Food:Location  \n",
      "4     Comfort & Facilities:Food:Staff  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "aspect_path = r\"C:\\Users\\andyb\\Desktop\\Coding Files\\PointView\\datasets\\aspect_based_dataset\\*.csv\"\n",
    "files = glob.glob(aspect_path)\n",
    "\n",
    "dataframes = [pd.read_csv(f) for f in files]\n",
    "df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andyb\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1_bai_hotel, results saved to C:\\Users\\andyb\\Desktop\\Coding Files\\PointView\\datasets\\sentiment_results\\1_bai_hotel\\1_bai_hotel_sentiment_analysis.csv\n",
      "Processed 2_dusit_thani_mactan, results saved to C:\\Users\\andyb\\Desktop\\Coding Files\\PointView\\datasets\\sentiment_results\\2_dusit_thani_mactan\\2_dusit_thani_mactan_sentiment_analysis.csv\n",
      "Processed 3_fairfield_by_marriott_cebu, results saved to C:\\Users\\andyb\\Desktop\\Coding Files\\PointView\\datasets\\sentiment_results\\3_fairfield_by_marriott_cebu\\3_fairfield_by_marriott_cebu_sentiment_analysis.csv\n",
      "Processed 4_jpark_island_resort_and_waterpark, results saved to C:\\Users\\andyb\\Desktop\\Coding Files\\PointView\\datasets\\sentiment_results\\4_jpark_island_resort_and_waterpark\\4_jpark_island_resort_and_waterpark_sentiment_analysis.csv\n",
      "Processed 5_seda_ayala_center_cebu, results saved to C:\\Users\\andyb\\Desktop\\Coding Files\\PointView\\datasets\\sentiment_results\\5_seda_ayala_center_cebu\\5_seda_ayala_center_cebu_sentiment_analysis.csv\n",
      "Processed 6_waterfront_hotel_and_casino, results saved to C:\\Users\\andyb\\Desktop\\Coding Files\\PointView\\datasets\\sentiment_results\\6_waterfront_hotel_and_casino\\6_waterfront_hotel_and_casino_sentiment_analysis.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define your specific KPIs\n",
    "specific_kpis = ['food', 'staff', 'comfort & facilities', 'value for money']\n",
    "\n",
    "# Base directory containing the hotel reviews\n",
    "base_dir = r\"C:\\Users\\andyb\\Desktop\\Coding Files\\PointView\\datasets\\hotel_aspect_based_dataset\"\n",
    "\n",
    "# Output directory to save the sentiment results\n",
    "output_dir = r\"C:\\Users\\andyb\\Desktop\\Coding Files\\PointView\\datasets\\sentiment_results\"\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Function to extract aspects from review content based on the KPIs\n",
    "def extract_aspects(review, aspects_list):\n",
    "    return [aspect for aspect in aspects_list if aspect.lower() in review.lower()]\n",
    "\n",
    "# Function to predict sentiment for an aspect\n",
    "def predict_sentiment(review, aspect):\n",
    "    input_text = f\"{aspect}: {review}\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    outputs = model(**inputs)\n",
    "    sentiment = torch.argmax(outputs.logits, dim=1).item()\n",
    "    return \"Positive\" if sentiment == 1 else \"Negative\"\n",
    "\n",
    "# Loop through each hotel directory and process the combined data\n",
    "for hotel_dir in os.listdir(base_dir):\n",
    "    hotel_path = os.path.join(base_dir, hotel_dir)\n",
    "    \n",
    "    if os.path.isdir(hotel_path):  # Check if it's a directory\n",
    "        combined_df = pd.DataFrame()  # Initialize an empty DataFrame to combine all files\n",
    "\n",
    "        # Combine all CSV files within the hotel directory\n",
    "        for csv_file in os.listdir(hotel_path):\n",
    "            if csv_file.endswith('.csv'):\n",
    "                file_path = os.path.join(hotel_path, csv_file)\n",
    "                temp_df = pd.read_csv(file_path)\n",
    "                combined_df = pd.concat([combined_df, temp_df], ignore_index=True)\n",
    "\n",
    "        # Perform the sentiment analysis on the combined DataFrame\n",
    "        combined_df['Aspects'] = combined_df['Review Content'].apply(lambda x: extract_aspects(x, specific_kpis))\n",
    "        combined_df['Aspects'] = combined_df['Aspects'].apply(lambda x: x if x else [])\n",
    "\n",
    "        combined_df['Sentiment_Results'] = combined_df.apply(\n",
    "            lambda row: {aspect: predict_sentiment(row['Review Content'], aspect) for aspect in row['Aspects']},\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        # Initialize dictionary to track positive/negative counts for each KPI\n",
    "        aspect_sentiments = {aspect: {'positive': 0, 'negative': 0} for aspect in specific_kpis}\n",
    "\n",
    "        # Count the positive and negative sentiments for each aspect\n",
    "        for index, row in combined_df.iterrows():\n",
    "            for aspect, sentiment in row['Sentiment_Results'].items():\n",
    "                if sentiment == \"Positive\":\n",
    "                    aspect_sentiments[aspect]['positive'] += 1\n",
    "                else:\n",
    "                    aspect_sentiments[aspect]['negative'] += 1\n",
    "\n",
    "        # Calculate sentiment percentages for each aspect\n",
    "        total_reviews = len(combined_df)\n",
    "        for aspect, counts in aspect_sentiments.items():\n",
    "            counts['positive_percent'] = (counts['positive'] / total_reviews) * 100\n",
    "            counts['negative_percent'] = (counts['negative'] / total_reviews) * 100\n",
    "\n",
    "        # Create a folder for the hotel in the output directory\n",
    "        hotel_output_dir = os.path.join(output_dir, hotel_dir)\n",
    "        if not os.path.exists(hotel_output_dir):\n",
    "            os.makedirs(hotel_output_dir)\n",
    "\n",
    "        # Save the sentiment analysis results to a CSV file\n",
    "        output_file_path = os.path.join(hotel_output_dir, f\"{hotel_dir}_sentiment_analysis.csv\")\n",
    "        output_df = pd.DataFrame(aspect_sentiments).T\n",
    "        output_df.to_csv(output_file_path)\n",
    "\n",
    "        print(f\"Processed {hotel_dir}, results saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the output directory where sentiment results are saved\n",
    "output_dir = r\"C:\\Users\\andyb\\Desktop\\Coding Files\\PointView\\datasets\\sentiment_results\"\n",
    "\n",
    "# Function to load and display sentiment results from all hotels\n",
    "def load_and_display_sentiment_results(output_dir):\n",
    "    for hotel_dir in os.listdir(output_dir):\n",
    "        hotel_path = os.path.join(output_dir, hotel_dir)\n",
    "        \n",
    "        if os.path.isdir(hotel_path):  # Check if it's a directory\n",
    "            for csv_file in os.listdir(hotel_path):\n",
    "                if csv_file.endswith('_sentiment_analysis.csv'):\n",
    "                    file_path = os.path.join(hotel_path, csv_file)\n",
    "                    \n",
    "                    # Load the CSV file into a DataFrame\n",
    "                    df = pd.read_csv(file_path, index_col=0)\n",
    "                    \n",
    "                    # Display the DataFrame\n",
    "                    print(f\"Sentiment Analysis for {hotel_dir}:\")\n",
    "                    print(df)\n",
    "                    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Call the function to display all sentiment results\n",
    "load_and_display_sentiment_results(output_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
