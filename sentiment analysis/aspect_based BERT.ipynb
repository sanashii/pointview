{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the data to be used for model fine tuning for consistent results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: C:\\Users\\andyb\\Desktop\\Coding Files\\PointView\\datasets\\aspect_based_dataset\\processed_batch_1.csv\n",
      "Processing file: C:\\Users\\andyb\\Desktop\\Coding Files\\PointView\\datasets\\aspect_based_dataset\\processed_batch_11.csv\n",
      "Processing file: C:\\Users\\andyb\\Desktop\\Coding Files\\PointView\\datasets\\aspect_based_dataset\\processed_batch_12.csv\n",
      "Processing file: C:\\Users\\andyb\\Desktop\\Coding Files\\PointView\\datasets\\aspect_based_dataset\\processed_batch_13.csv\n",
      "Processing file: C:\\Users\\andyb\\Desktop\\Coding Files\\PointView\\datasets\\aspect_based_dataset\\processed_batch_14.csv\n",
      "Processing file: C:\\Users\\andyb\\Desktop\\Coding Files\\PointView\\datasets\\aspect_based_dataset\\processed_batch_15.csv\n",
      "Processing file: C:\\Users\\andyb\\Desktop\\Coding Files\\PointView\\datasets\\aspect_based_dataset\\processed_batch_16.csv\n",
      "Processing file: C:\\Users\\andyb\\Desktop\\Coding Files\\PointView\\datasets\\aspect_based_dataset\\processed_batch_17.csv\n",
      "Processing file: C:\\Users\\andyb\\Desktop\\Coding Files\\PointView\\datasets\\aspect_based_dataset\\processed_batch_18.csv\n",
      "Processing file: C:\\Users\\andyb\\Desktop\\Coding Files\\PointView\\datasets\\aspect_based_dataset\\processed_batch_19.csv\n",
      "Processing file: C:\\Users\\andyb\\Desktop\\Coding Files\\PointView\\datasets\\aspect_based_dataset\\processed_batch_2.csv\n",
      "Processing file: C:\\Users\\andyb\\Desktop\\Coding Files\\PointView\\datasets\\aspect_based_dataset\\processed_batch_20.csv\n",
      "Processing file: C:\\Users\\andyb\\Desktop\\Coding Files\\PointView\\datasets\\aspect_based_dataset\\processed_batch_21.csv\n",
      "Processing file: C:\\Users\\andyb\\Desktop\\Coding Files\\PointView\\datasets\\aspect_based_dataset\\processed_batch_22.csv\n",
      "Processing file: C:\\Users\\andyb\\Desktop\\Coding Files\\PointView\\datasets\\aspect_based_dataset\\processed_batch_23.csv\n",
      "Processing file: C:\\Users\\andyb\\Desktop\\Coding Files\\PointView\\datasets\\aspect_based_dataset\\processed_batch_24.csv\n",
      "Processing file: C:\\Users\\andyb\\Desktop\\Coding Files\\PointView\\datasets\\aspect_based_dataset\\processed_batch_25.csv\n",
      "Processing file: C:\\Users\\andyb\\Desktop\\Coding Files\\PointView\\datasets\\aspect_based_dataset\\processed_batch_26.csv\n",
      "Processing file: C:\\Users\\andyb\\Desktop\\Coding Files\\PointView\\datasets\\aspect_based_dataset\\processed_batch_27.csv\n",
      "Processing file: C:\\Users\\andyb\\Desktop\\Coding Files\\PointView\\datasets\\aspect_based_dataset\\processed_batch_28.csv\n",
      "Processing file: C:\\Users\\andyb\\Desktop\\Coding Files\\PointView\\datasets\\aspect_based_dataset\\processed_batch_29.csv\n",
      "Processing file: C:\\Users\\andyb\\Desktop\\Coding Files\\PointView\\datasets\\aspect_based_dataset\\processed_batch_3.csv\n",
      "Processing file: C:\\Users\\andyb\\Desktop\\Coding Files\\PointView\\datasets\\aspect_based_dataset\\processed_batch_30.csv\n",
      "Processing file: C:\\Users\\andyb\\Desktop\\Coding Files\\PointView\\datasets\\aspect_based_dataset\\processed_batch_4.csv\n",
      "Processing file: C:\\Users\\andyb\\Desktop\\Coding Files\\PointView\\datasets\\aspect_based_dataset\\processed_batch_5.csv\n",
      "Processing file: C:\\Users\\andyb\\Desktop\\Coding Files\\PointView\\datasets\\aspect_based_dataset\\processed_batch_6.csv\n",
      "Processing file: C:\\Users\\andyb\\Desktop\\Coding Files\\PointView\\datasets\\aspect_based_dataset\\processed_batch_7.csv\n",
      "Processing file: C:\\Users\\andyb\\Desktop\\Coding Files\\PointView\\datasets\\aspect_based_dataset\\processed_batch_8.csv\n",
      "Processing file: C:\\Users\\andyb\\Desktop\\Coding Files\\PointView\\datasets\\aspect_based_dataset\\processed_batch_9.csv\n",
      "Exploded DataFrame:\n",
      "                                             Opinion                Aspect  \\\n",
      "0  We stayed for a week and could not fault it at...                 Staff   \n",
      "1  We stayed for a week and could not fault it at...              Location   \n",
      "2  This resort is beautiful. The rooms are fabulo...                  Food   \n",
      "3  This resort is beautiful. The rooms are fabulo...  Comfort & Facilities   \n",
      "4  i never fail to visit Shangrila Boracay eveyti...              Location   \n",
      "\n",
      "  Sentiment  \n",
      "0  Positive  \n",
      "1  Positive  \n",
      "2  Positive  \n",
      "3  Positive  \n",
      "4  Positive  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "import ast\n",
    "import glob\n",
    "\n",
    "# Load the dataset\n",
    "aspect_path = r\"C:\\Users\\andyb\\Desktop\\Coding Files\\PointView\\datasets\\aspect_based_dataset\\*.csv\"\n",
    "files = glob.glob(aspect_path)\n",
    "\n",
    "# Initialize an empty list to store dataframes\n",
    "dataframes = []\n",
    "\n",
    "# Iterate through each CSV file and load it into a DataFrame\n",
    "for f in files:\n",
    "    print(f\"Processing file: {f}\")\n",
    "    df_temp = pd.read_csv(f)\n",
    "    \n",
    "    # Ensure 'Classification' is string and handle missing values\n",
    "    df_temp['Classification'] = df_temp['Classification'].fillna('').astype(str)\n",
    "    \n",
    "    # Append the dataframe to the list\n",
    "    dataframes.append(df_temp)\n",
    "\n",
    "# Concatenate all the DataFrames into one\n",
    "df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Remove duplicates based on the 'Opinion' column (or a combination of columns if needed)\n",
    "df = df.drop_duplicates(subset=['Opinion'])\n",
    "\n",
    "# Define function to get sentiment using TextBlob\n",
    "def get_sentiment(opinion):\n",
    "    analysis = TextBlob(opinion)\n",
    "    # Determine the polarity (-1 is very negative, +1 is very positive)\n",
    "    if analysis.sentiment.polarity > 0:\n",
    "        return 'Positive'\n",
    "    elif analysis.sentiment.polarity < 0:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "# Define function to label aspects with sentiments\n",
    "def label_aspects(row):\n",
    "    sentiments = {}\n",
    "    aspects = row['Classification'].split(':')\n",
    "    for aspect in aspects:\n",
    "        sentiment = get_sentiment(row['Opinion'])\n",
    "        sentiments[aspect] = sentiment\n",
    "    return sentiments\n",
    "\n",
    "# Apply the labeling function to each row\n",
    "df['Aspect_Sentiment'] = df.apply(label_aspects, axis=1)\n",
    "\n",
    "# Convert 'Aspect_Sentiment' to actual dictionaries if they are strings\n",
    "df['Aspect_Sentiment'] = df['Aspect_Sentiment'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "# Manually explode the dictionary into separate rows\n",
    "rows = []\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    for aspect, sentiment in row['Aspect_Sentiment'].items():\n",
    "        rows.append({\n",
    "            'Opinion': row['Opinion'],\n",
    "            'Aspect': aspect,\n",
    "            'Sentiment': sentiment\n",
    "        })\n",
    "\n",
    "# Create a new DataFrame with the exploded aspects and sentiments\n",
    "exploded_df = pd.DataFrame(rows)\n",
    "\n",
    "# Remove duplicates in the exploded DataFrame if needed\n",
    "exploded_df = exploded_df.drop_duplicates()\n",
    "\n",
    "# Check the final output\n",
    "print(\"Exploded DataFrame:\")\n",
    "print(exploded_df.head())\n",
    "\n",
    "# Save the labeled dataset\n",
    "exploded_df.to_csv(r'C:\\Users\\andyb\\Desktop\\Coding Files\\PointView\\datasets\\labeled_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "# import torch\n",
    "# import numpy as np\n",
    "# import random\n",
    "# from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "# import torch.backends.cudnn as cudnn\n",
    "\n",
    "# # Set random seed for reproducibility\n",
    "# def set_seed(seed=42):\n",
    "#     random.seed(seed)\n",
    "#     np.random.seed(seed)\n",
    "#     torch.manual_seed(seed)\n",
    "#     if torch.cuda.is_available():\n",
    "#         torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# set_seed(42)\n",
    "\n",
    "# # Ensure deterministic behavior\n",
    "# cudnn.deterministic = True\n",
    "# cudnn.benchmark = False\n",
    "\n",
    "# # Check if GPU is available and set the device accordingly\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# print(f'Using device: {device}')\n",
    "\n",
    "# # Load pre-trained model and tokenizer\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# # Move the model to the GPU\n",
    "# model.to(device)\n",
    "\n",
    "# # Set model to evaluation mode to disable dropout\n",
    "# model.eval()\n",
    "\n",
    "# # Define your specific KPIs\n",
    "# specific_kpis = ['food', 'staff', 'comfort', 'facilities', 'value for money']\n",
    "\n",
    "# # Load the labeled dataset (for fine-tuning)\n",
    "# labeled_data_path = r\"C:\\Users\\andyb\\Desktop\\Coding Files\\pointview\\datasets\\labeled_dataset.csv\"\n",
    "# df = pd.read_csv(labeled_data_path)\n",
    "\n",
    "# # Convert the sentiment labels to integers\n",
    "# label_mapping = {'Positive': 1, 'Negative': 0}\n",
    "# df['Sentiment'] = df['Sentiment'].map(label_mapping)\n",
    "\n",
    "# # Identify rows with NaN values after mapping\n",
    "# nan_rows = df[df['Sentiment'].isna()]\n",
    "# print(\"Rows with NaN values in 'Sentiment':\")\n",
    "# print(nan_rows)\n",
    "\n",
    "# # Handle NaN values\n",
    "# df = df.dropna(subset=['Sentiment'])  # Option 1: Drop rows with NaN values\n",
    "\n",
    "# # Ensure the labels are of type int\n",
    "# df['Sentiment'] = df['Sentiment'].astype(int)\n",
    "\n",
    "# # Inspect the label values and their data types\n",
    "# print(\"Unique values in label column:\", df['Sentiment'].unique())\n",
    "# print(\"Data type of label column:\", df['Sentiment'].dtype)\n",
    "\n",
    "# # Tokenize the dataset\n",
    "# train_encodings = tokenizer(df['Opinion'].tolist(), truncation=True, padding=True, max_length=512)\n",
    "# train_labels = df['Sentiment'].tolist()  # These should now be integers\n",
    "\n",
    "# # Create a custom Dataset class\n",
    "# class ReviewDataset(torch.utils.data.Dataset):\n",
    "#     def __init__(self, encodings, labels):\n",
    "#         self.encodings = encodings\n",
    "#         self.labels = labels\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         item = {key: torch.tensor(val[idx], dtype=torch.long) for key, val in self.encodings.items()}\n",
    "#         item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "#         return item\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.labels)\n",
    "\n",
    "# train_dataset = ReviewDataset(train_encodings, train_labels)\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir='./results',          # output directory\n",
    "#     num_train_epochs=3,              # total number of training epochs\n",
    "#     per_device_train_batch_size=16,  # batch size per device during training\n",
    "#     warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "#     weight_decay=0.01,               # strength of weight decay\n",
    "#     logging_dir='./logs',            # directory for storing logs\n",
    "#     logging_steps=10,\n",
    "# )\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "#     args=training_args,                  # training arguments, defined above\n",
    "#     train_dataset=train_dataset          # training dataset\n",
    "# )\n",
    "\n",
    "# # Train the model\n",
    "# trainer.train()\n",
    "\n",
    "# # Save the fine-tuned model\n",
    "# model.save_pretrained(\"./fine_tuned_model\")\n",
    "# tokenizer.save_pretrained(\"./fine_tuned_model\")\n",
    "\n",
    "\n",
    "# # Load the fine-tuned model\n",
    "# model = BertForSequenceClassification.from_pretrained('./fine_tuned_model')\n",
    "# tokenizer = BertTokenizer.from_pretrained('./fine_tuned_model')\n",
    "\n",
    "# # Move the model to the GPU\n",
    "# model.to(device)\n",
    "\n",
    "# # Set model to evaluation mode\n",
    "# model.eval()\n",
    "\n",
    "# # Base directory containing the hotel reviews\n",
    "# base_dir = r\"C:\\Users\\andyb\\Desktop\\Coding Files\\PointView\\datasets\\hotel_aspect_based_dataset\"\n",
    "\n",
    "# # Output directory to save the sentiment results\n",
    "# output_dir = r\"C:\\Users\\andyb\\Desktop\\Coding Files\\PointView\\datasets\\sentiment_results\"\n",
    "\n",
    "# if not os.path.exists(output_dir):\n",
    "#     os.makedirs(output_dir)\n",
    "\n",
    "# # Function to extract aspects from review content based on the KPIs\n",
    "# def extract_aspects(review, aspects_list):\n",
    "#     return [aspect for aspect in aspects_list if aspect.lower() in review.lower()]\n",
    "\n",
    "# # Check if the model is on the GPU\n",
    "# print(f\"Model is on device: {next(model.parameters()).device}\")\n",
    "\n",
    "# # When you're processing inputs:\n",
    "# def predict_sentiment(review, aspect):\n",
    "#     input_text = f\"{aspect}: {review}\"\n",
    "#     inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)  # Move inputs to GPU\n",
    "#     print(f\"Inputs are on device: {inputs['input_ids'].device}\")  # Check if the inputs are on GPU\n",
    "#     outputs = model(**inputs)\n",
    "#     sentiment = torch.argmax(outputs.logits, dim=1).item()\n",
    "#     return \"Positive\" if sentiment == 1 else \"Negative\"\n",
    "\n",
    "# # Loop through each hotel directory and process the combined data\n",
    "# for hotel_dir in os.listdir(base_dir):\n",
    "#     hotel_path = os.path.join(base_dir, hotel_dir)\n",
    "    \n",
    "#     if os.path.isdir(hotel_path):  # Check if it's a directory\n",
    "#         combined_df = pd.DataFrame()  # Initialize an empty DataFrame to combine all files\n",
    "\n",
    "#         # Combine all CSV files within the hotel directory\n",
    "#         for csv_file in os.listdir(hotel_path):\n",
    "#             if csv_file.endswith('.csv'):\n",
    "#                 file_path = os.path.join(hotel_path, csv_file)\n",
    "#                 temp_df = pd.read_csv(file_path)\n",
    "#                 combined_df = pd.concat([combined_df, temp_df], ignore_index=True)\n",
    "\n",
    "#         # Perform the sentiment analysis on the combined DataFrame\n",
    "#         combined_df['Aspects'] = combined_df['Review Content'].apply(lambda x: extract_aspects(x, specific_kpis))\n",
    "#         combined_df['Aspects'] = combined_df['Aspects'].apply(lambda x: x if x else [])\n",
    "\n",
    "#         combined_df['Sentiment_Results'] = combined_df.apply(\n",
    "#             lambda row: {aspect: predict_sentiment(row['Review Content'], aspect) for aspect in row['Aspects']},\n",
    "#             axis=1\n",
    "#         )\n",
    "\n",
    "#         # Initialize dictionary to track positive/negative counts for each KPI\n",
    "#         aspect_sentiments = {aspect: {'positive': 0, 'negative': 0} for aspect in specific_kpis}\n",
    "\n",
    "#         # Count the positive and negative sentiments for each aspect\n",
    "#         for index, row in combined_df.iterrows():\n",
    "#             for aspect, sentiment in row['Sentiment_Results'].items():\n",
    "#                 if sentiment == \"Positive\":\n",
    "#                     aspect_sentiments[aspect]['positive'] += 1\n",
    "#                 else:\n",
    "#                     aspect_sentiments[aspect]['negative'] += 1\n",
    "\n",
    "#         # Calculate sentiment percentages for each aspect\n",
    "#         total_reviews = len(combined_df)\n",
    "#         for aspect, counts in aspect_sentiments.items():\n",
    "#             counts['positive_percent'] = (counts['positive'] / total_reviews) * 100\n",
    "#             counts['negative_percent'] = (counts['negative'] / total_reviews) * 100\n",
    "\n",
    "#         # Create a folder for the hotel in the output directory\n",
    "#         hotel_output_dir = os.path.join(output_dir, hotel_dir)\n",
    "#         if not os.path.exists(hotel_output_dir):\n",
    "#             os.makedirs(hotel_output_dir)\n",
    "\n",
    "#         # Save the sentiment analysis results to a CSV file\n",
    "#         output_file_path = os.path.join(hotel_output_dir, f\"{hotel_dir}_sentiment_analysis.csv\")\n",
    "#         output_df = pd.DataFrame(aspect_sentiments).T\n",
    "#         output_df.to_csv(output_file_path)\n",
    "\n",
    "#         print(f\"Processed {hotel_dir}, results saved to {output_file_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\andyb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\andyb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\andyb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec7037340dff43469e99eaef054bd445",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2493 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0411, 'grad_norm': 0.006221645046025515, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.01}\n",
      "{'loss': 0.0979, 'grad_norm': 0.00314724282361567, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.02}\n",
      "{'loss': 0.0482, 'grad_norm': 4.707668304443359, 'learning_rate': 3e-06, 'epoch': 0.04}\n",
      "{'loss': 0.0446, 'grad_norm': 0.29841139912605286, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.05}\n",
      "{'loss': 0.0471, 'grad_norm': 0.02807762287557125, 'learning_rate': 5e-06, 'epoch': 0.06}\n",
      "{'loss': 0.0027, 'grad_norm': 0.02131219021975994, 'learning_rate': 6e-06, 'epoch': 0.07}\n",
      "{'loss': 0.0561, 'grad_norm': 0.01366171520203352, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.08}\n",
      "{'loss': 0.0033, 'grad_norm': 0.00858778040856123, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.1}\n",
      "{'loss': 0.0006, 'grad_norm': 0.033091384917497635, 'learning_rate': 9e-06, 'epoch': 0.11}\n",
      "{'loss': 0.0292, 'grad_norm': 0.03683312609791756, 'learning_rate': 1e-05, 'epoch': 0.12}\n",
      "{'loss': 0.0473, 'grad_norm': 0.1262229084968567, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.13}\n",
      "{'loss': 0.0064, 'grad_norm': 0.00899218488484621, 'learning_rate': 1.2e-05, 'epoch': 0.14}\n",
      "{'loss': 0.0316, 'grad_norm': 0.012853843159973621, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.16}\n",
      "{'loss': 0.0629, 'grad_norm': 6.3197784423828125, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.17}\n",
      "{'loss': 0.0004, 'grad_norm': 0.05006306618452072, 'learning_rate': 1.5e-05, 'epoch': 0.18}\n",
      "{'loss': 0.0017, 'grad_norm': 0.06178886815905571, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.19}\n",
      "{'loss': 0.0003, 'grad_norm': 0.004587174858897924, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.2}\n",
      "{'loss': 0.1089, 'grad_norm': 0.01706543192267418, 'learning_rate': 1.8e-05, 'epoch': 0.22}\n",
      "{'loss': 0.0015, 'grad_norm': 0.16069139540195465, 'learning_rate': 1.9e-05, 'epoch': 0.23}\n",
      "{'loss': 0.0012, 'grad_norm': 0.04177101328969002, 'learning_rate': 2e-05, 'epoch': 0.24}\n",
      "{'loss': 0.0441, 'grad_norm': 0.013922288082540035, 'learning_rate': 2.1e-05, 'epoch': 0.25}\n",
      "{'loss': 0.0339, 'grad_norm': 0.0033679662737995386, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.26}\n",
      "{'loss': 0.1401, 'grad_norm': 0.05380304530262947, 'learning_rate': 2.3000000000000003e-05, 'epoch': 0.28}\n",
      "{'loss': 0.048, 'grad_norm': 0.23831652104854584, 'learning_rate': 2.4e-05, 'epoch': 0.29}\n",
      "{'loss': 0.0063, 'grad_norm': 10.819092750549316, 'learning_rate': 2.5e-05, 'epoch': 0.3}\n",
      "{'loss': 0.0233, 'grad_norm': 0.03322892263531685, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.31}\n",
      "{'loss': 0.0004, 'grad_norm': 0.026654008775949478, 'learning_rate': 2.7000000000000002e-05, 'epoch': 0.32}\n",
      "{'loss': 0.0345, 'grad_norm': 0.14213381707668304, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.34}\n",
      "{'loss': 0.0413, 'grad_norm': 0.0028678474482148886, 'learning_rate': 2.9e-05, 'epoch': 0.35}\n",
      "{'loss': 0.0024, 'grad_norm': 0.61174076795578, 'learning_rate': 3e-05, 'epoch': 0.36}\n",
      "{'loss': 0.0422, 'grad_norm': 0.01277514174580574, 'learning_rate': 3.1e-05, 'epoch': 0.37}\n",
      "{'loss': 0.037, 'grad_norm': 0.04048207402229309, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.39}\n",
      "{'loss': 0.0614, 'grad_norm': 13.708535194396973, 'learning_rate': 3.3e-05, 'epoch': 0.4}\n",
      "{'loss': 0.0143, 'grad_norm': 0.08032707124948502, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.41}\n",
      "{'loss': 0.0348, 'grad_norm': 0.05428869277238846, 'learning_rate': 3.5e-05, 'epoch': 0.42}\n",
      "{'loss': 0.0714, 'grad_norm': 0.035459697246551514, 'learning_rate': 3.6e-05, 'epoch': 0.43}\n",
      "{'loss': 0.0044, 'grad_norm': 0.004596209619194269, 'learning_rate': 3.7e-05, 'epoch': 0.45}\n",
      "{'loss': 0.0824, 'grad_norm': 0.04499136283993721, 'learning_rate': 3.8e-05, 'epoch': 0.46}\n",
      "{'loss': 0.0635, 'grad_norm': 0.04967861622571945, 'learning_rate': 3.9000000000000006e-05, 'epoch': 0.47}\n",
      "{'loss': 0.0256, 'grad_norm': 0.012751384638249874, 'learning_rate': 4e-05, 'epoch': 0.48}\n",
      "{'loss': 0.0167, 'grad_norm': 0.13823354244232178, 'learning_rate': 4.1e-05, 'epoch': 0.49}\n",
      "{'loss': 0.0011, 'grad_norm': 0.25146931409835815, 'learning_rate': 4.2e-05, 'epoch': 0.51}\n",
      "{'loss': 0.0007, 'grad_norm': 0.004338025115430355, 'learning_rate': 4.3e-05, 'epoch': 0.52}\n",
      "{'loss': 0.0828, 'grad_norm': 0.027927560731768608, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.53}\n",
      "{'loss': 0.1103, 'grad_norm': 0.03488237038254738, 'learning_rate': 4.5e-05, 'epoch': 0.54}\n",
      "{'loss': 0.0486, 'grad_norm': 0.09373383224010468, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.55}\n",
      "{'loss': 0.0798, 'grad_norm': 3.1390514373779297, 'learning_rate': 4.7e-05, 'epoch': 0.57}\n",
      "{'loss': 0.0023, 'grad_norm': 0.09049327671527863, 'learning_rate': 4.8e-05, 'epoch': 0.58}\n",
      "{'loss': 0.032, 'grad_norm': 0.18727681040763855, 'learning_rate': 4.9e-05, 'epoch': 0.59}\n",
      "{'loss': 0.064, 'grad_norm': 1.4903993606567383, 'learning_rate': 5e-05, 'epoch': 0.6}\n",
      "{'loss': 0.0715, 'grad_norm': 0.43618860840797424, 'learning_rate': 4.9749121926743605e-05, 'epoch': 0.61}\n",
      "{'loss': 0.0008, 'grad_norm': 0.014897438697516918, 'learning_rate': 4.949824385348721e-05, 'epoch': 0.63}\n",
      "{'loss': 0.0777, 'grad_norm': 5.064545631408691, 'learning_rate': 4.924736578023081e-05, 'epoch': 0.64}\n",
      "{'loss': 0.0464, 'grad_norm': 0.04644561931490898, 'learning_rate': 4.899648770697441e-05, 'epoch': 0.65}\n",
      "{'loss': 0.0367, 'grad_norm': 2.2338974475860596, 'learning_rate': 4.8745609633718014e-05, 'epoch': 0.66}\n",
      "{'loss': 0.1321, 'grad_norm': 0.29255416989326477, 'learning_rate': 4.849473156046162e-05, 'epoch': 0.67}\n",
      "{'loss': 0.0513, 'grad_norm': 0.07055120915174484, 'learning_rate': 4.824385348720522e-05, 'epoch': 0.69}\n",
      "{'loss': 0.0022, 'grad_norm': 0.026735566556453705, 'learning_rate': 4.799297541394882e-05, 'epoch': 0.7}\n",
      "{'loss': 0.0806, 'grad_norm': 0.01511860080063343, 'learning_rate': 4.774209734069243e-05, 'epoch': 0.71}\n",
      "{'loss': 0.022, 'grad_norm': 0.013898782432079315, 'learning_rate': 4.7491219267436025e-05, 'epoch': 0.72}\n",
      "{'loss': 0.0216, 'grad_norm': 7.666818618774414, 'learning_rate': 4.7240341194179634e-05, 'epoch': 0.73}\n",
      "{'loss': 0.0002, 'grad_norm': 0.007378341630101204, 'learning_rate': 4.698946312092323e-05, 'epoch': 0.75}\n",
      "{'loss': 0.0786, 'grad_norm': 4.737848281860352, 'learning_rate': 4.673858504766684e-05, 'epoch': 0.76}\n",
      "{'loss': 0.052, 'grad_norm': 2.153998851776123, 'learning_rate': 4.648770697441044e-05, 'epoch': 0.77}\n",
      "{'loss': 0.0417, 'grad_norm': 2.002915859222412, 'learning_rate': 4.623682890115404e-05, 'epoch': 0.78}\n",
      "{'loss': 0.1154, 'grad_norm': 1.578626036643982, 'learning_rate': 4.5985950827897646e-05, 'epoch': 0.79}\n",
      "{'loss': 0.1337, 'grad_norm': 16.06147003173828, 'learning_rate': 4.573507275464125e-05, 'epoch': 0.81}\n",
      "{'loss': 0.0111, 'grad_norm': 0.029536275193095207, 'learning_rate': 4.548419468138485e-05, 'epoch': 0.82}\n",
      "{'loss': 0.0027, 'grad_norm': 0.014637621119618416, 'learning_rate': 4.523331660812845e-05, 'epoch': 0.83}\n",
      "{'loss': 0.1795, 'grad_norm': 2.068962335586548, 'learning_rate': 4.4982438534872055e-05, 'epoch': 0.84}\n",
      "{'loss': 0.0495, 'grad_norm': 0.19779135286808014, 'learning_rate': 4.473156046161566e-05, 'epoch': 0.85}\n",
      "{'loss': 0.0756, 'grad_norm': 0.03932924568653107, 'learning_rate': 4.448068238835926e-05, 'epoch': 0.87}\n",
      "{'loss': 0.0439, 'grad_norm': 0.0411754734814167, 'learning_rate': 4.422980431510286e-05, 'epoch': 0.88}\n",
      "{'loss': 0.088, 'grad_norm': 15.688368797302246, 'learning_rate': 4.3978926241846464e-05, 'epoch': 0.89}\n",
      "{'loss': 0.0434, 'grad_norm': 0.03259610757231712, 'learning_rate': 4.372804816859007e-05, 'epoch': 0.9}\n",
      "{'loss': 0.1254, 'grad_norm': 0.09888837486505508, 'learning_rate': 4.347717009533367e-05, 'epoch': 0.91}\n",
      "{'loss': 0.0885, 'grad_norm': 12.898005485534668, 'learning_rate': 4.322629202207728e-05, 'epoch': 0.93}\n",
      "{'loss': 0.0367, 'grad_norm': 0.14064419269561768, 'learning_rate': 4.2975413948820874e-05, 'epoch': 0.94}\n",
      "{'loss': 0.02, 'grad_norm': 0.0734332874417305, 'learning_rate': 4.2724535875564476e-05, 'epoch': 0.95}\n",
      "{'loss': 0.0411, 'grad_norm': 0.07116107642650604, 'learning_rate': 4.247365780230808e-05, 'epoch': 0.96}\n",
      "{'loss': 0.1808, 'grad_norm': 3.996797800064087, 'learning_rate': 4.222277972905168e-05, 'epoch': 0.97}\n",
      "{'loss': 0.0392, 'grad_norm': 0.13628575205802917, 'learning_rate': 4.197190165579529e-05, 'epoch': 0.99}\n",
      "{'loss': 0.0427, 'grad_norm': 0.10680898278951645, 'learning_rate': 4.1721023582538885e-05, 'epoch': 1.0}\n",
      "{'loss': 0.0037, 'grad_norm': 0.07608069479465485, 'learning_rate': 4.1470145509282494e-05, 'epoch': 1.01}\n",
      "{'loss': 0.1105, 'grad_norm': 106.84835052490234, 'learning_rate': 4.1219267436026096e-05, 'epoch': 1.02}\n",
      "{'loss': 0.054, 'grad_norm': 0.08950325101613998, 'learning_rate': 4.096838936276969e-05, 'epoch': 1.03}\n",
      "{'loss': 0.1985, 'grad_norm': 4.160935878753662, 'learning_rate': 4.07175112895133e-05, 'epoch': 1.05}\n",
      "{'loss': 0.1421, 'grad_norm': 13.389608383178711, 'learning_rate': 4.04666332162569e-05, 'epoch': 1.06}\n",
      "{'loss': 0.0578, 'grad_norm': 3.2115235328674316, 'learning_rate': 4.0215755143000506e-05, 'epoch': 1.07}\n",
      "{'loss': 0.1038, 'grad_norm': 10.064730644226074, 'learning_rate': 3.996487706974411e-05, 'epoch': 1.08}\n",
      "{'loss': 0.0055, 'grad_norm': 0.17388634383678436, 'learning_rate': 3.971399899648771e-05, 'epoch': 1.1}\n",
      "{'loss': 0.0639, 'grad_norm': 0.11136869341135025, 'learning_rate': 3.946312092323131e-05, 'epoch': 1.11}\n",
      "{'loss': 0.0027, 'grad_norm': 0.07646339386701584, 'learning_rate': 3.9212242849974915e-05, 'epoch': 1.12}\n",
      "{'loss': 0.002, 'grad_norm': 0.054346632212400436, 'learning_rate': 3.896136477671852e-05, 'epoch': 1.13}\n",
      "{'loss': 0.0015, 'grad_norm': 0.03809439018368721, 'learning_rate': 3.871048670346212e-05, 'epoch': 1.14}\n",
      "{'loss': 0.0009, 'grad_norm': 0.024990152567625046, 'learning_rate': 3.845960863020572e-05, 'epoch': 1.16}\n",
      "{'loss': 0.0381, 'grad_norm': 0.009517965838313103, 'learning_rate': 3.8208730556949324e-05, 'epoch': 1.17}\n",
      "{'loss': 0.0336, 'grad_norm': 18.045656204223633, 'learning_rate': 3.7957852483692926e-05, 'epoch': 1.18}\n",
      "{'loss': 0.1136, 'grad_norm': 0.019642459228634834, 'learning_rate': 3.770697441043653e-05, 'epoch': 1.19}\n",
      "{'loss': 0.0702, 'grad_norm': 0.21242377161979675, 'learning_rate': 3.745609633718013e-05, 'epoch': 1.2}\n",
      "{'loss': 0.0205, 'grad_norm': 13.538508415222168, 'learning_rate': 3.720521826392373e-05, 'epoch': 1.22}\n",
      "{'loss': 0.0339, 'grad_norm': 0.0524638406932354, 'learning_rate': 3.6954340190667336e-05, 'epoch': 1.23}\n",
      "{'loss': 0.0009, 'grad_norm': 0.027654139325022697, 'learning_rate': 3.6703462117410945e-05, 'epoch': 1.24}\n",
      "{'loss': 0.0652, 'grad_norm': 0.11141443997621536, 'learning_rate': 3.645258404415454e-05, 'epoch': 1.25}\n",
      "{'loss': 0.0732, 'grad_norm': 9.056594848632812, 'learning_rate': 3.620170597089815e-05, 'epoch': 1.26}\n",
      "{'loss': 0.0102, 'grad_norm': 0.004264135379344225, 'learning_rate': 3.5950827897641745e-05, 'epoch': 1.28}\n",
      "{'loss': 0.1505, 'grad_norm': 1.8009438514709473, 'learning_rate': 3.569994982438535e-05, 'epoch': 1.29}\n",
      "{'loss': 0.084, 'grad_norm': 0.17641858756542206, 'learning_rate': 3.5449071751128956e-05, 'epoch': 1.3}\n",
      "{'loss': 0.0087, 'grad_norm': 0.12039723247289658, 'learning_rate': 3.519819367787255e-05, 'epoch': 1.31}\n",
      "{'loss': 0.0696, 'grad_norm': 7.100545883178711, 'learning_rate': 3.494731560461616e-05, 'epoch': 1.32}\n",
      "{'loss': 0.0832, 'grad_norm': 1.9596928358078003, 'learning_rate': 3.469643753135976e-05, 'epoch': 1.34}\n",
      "{'loss': 0.1132, 'grad_norm': 0.5479295253753662, 'learning_rate': 3.4445559458103365e-05, 'epoch': 1.35}\n",
      "{'loss': 0.0719, 'grad_norm': 0.20013798773288727, 'learning_rate': 3.419468138484697e-05, 'epoch': 1.36}\n",
      "{'loss': 0.1268, 'grad_norm': 0.2340572625398636, 'learning_rate': 3.394380331159056e-05, 'epoch': 1.37}\n",
      "{'loss': 0.1113, 'grad_norm': 5.736289024353027, 'learning_rate': 3.369292523833417e-05, 'epoch': 1.38}\n",
      "{'loss': 0.0619, 'grad_norm': 0.20137523114681244, 'learning_rate': 3.3442047165077775e-05, 'epoch': 1.4}\n",
      "{'loss': 0.0236, 'grad_norm': 0.39408478140830994, 'learning_rate': 3.319116909182138e-05, 'epoch': 1.41}\n",
      "{'loss': 0.0419, 'grad_norm': 0.7949578762054443, 'learning_rate': 3.294029101856498e-05, 'epoch': 1.42}\n",
      "{'loss': 0.0023, 'grad_norm': 0.06852902472019196, 'learning_rate': 3.268941294530858e-05, 'epoch': 1.43}\n",
      "{'loss': 0.0444, 'grad_norm': 0.0641467347741127, 'learning_rate': 3.2438534872052184e-05, 'epoch': 1.44}\n",
      "{'loss': 0.0745, 'grad_norm': 0.10466533154249191, 'learning_rate': 3.2187656798795786e-05, 'epoch': 1.46}\n",
      "{'loss': 0.0808, 'grad_norm': 0.16384924948215485, 'learning_rate': 3.193677872553939e-05, 'epoch': 1.47}\n",
      "{'loss': 0.0421, 'grad_norm': 0.09571743756532669, 'learning_rate': 3.168590065228299e-05, 'epoch': 1.48}\n",
      "{'loss': 0.1072, 'grad_norm': 0.08965514600276947, 'learning_rate': 3.143502257902659e-05, 'epoch': 1.49}\n",
      "{'loss': 0.0967, 'grad_norm': 0.11444992572069168, 'learning_rate': 3.1184144505770195e-05, 'epoch': 1.5}\n",
      "{'loss': 0.0114, 'grad_norm': 0.15565192699432373, 'learning_rate': 3.0933266432513804e-05, 'epoch': 1.52}\n",
      "{'loss': 0.006, 'grad_norm': 0.07599001377820969, 'learning_rate': 3.06823883592574e-05, 'epoch': 1.53}\n",
      "{'loss': 0.0103, 'grad_norm': 0.06800514459609985, 'learning_rate': 3.0431510286001002e-05, 'epoch': 1.54}\n",
      "{'loss': 0.0376, 'grad_norm': 0.07809386402368546, 'learning_rate': 3.0180632212744608e-05, 'epoch': 1.55}\n",
      "{'loss': 0.1114, 'grad_norm': 2.247450590133667, 'learning_rate': 2.992975413948821e-05, 'epoch': 1.56}\n",
      "{'loss': 0.029, 'grad_norm': 0.07431894540786743, 'learning_rate': 2.9678876066231816e-05, 'epoch': 1.58}\n",
      "{'loss': 0.0815, 'grad_norm': 0.19409799575805664, 'learning_rate': 2.9427997992975415e-05, 'epoch': 1.59}\n",
      "{'loss': 0.0535, 'grad_norm': 0.0797886922955513, 'learning_rate': 2.917711991971902e-05, 'epoch': 1.6}\n",
      "{'loss': 0.0386, 'grad_norm': 0.07027143239974976, 'learning_rate': 2.892624184646262e-05, 'epoch': 1.61}\n",
      "{'loss': 0.0422, 'grad_norm': 0.06471667438745499, 'learning_rate': 2.8675363773206222e-05, 'epoch': 1.62}\n",
      "{'loss': 0.0795, 'grad_norm': 0.06114557385444641, 'learning_rate': 2.8424485699949827e-05, 'epoch': 1.64}\n",
      "{'loss': 0.0396, 'grad_norm': 0.06552106142044067, 'learning_rate': 2.8173607626693426e-05, 'epoch': 1.65}\n",
      "{'loss': 0.0734, 'grad_norm': 0.13158628344535828, 'learning_rate': 2.7922729553437032e-05, 'epoch': 1.66}\n",
      "{'loss': 0.003, 'grad_norm': 0.07322005927562714, 'learning_rate': 2.7671851480180634e-05, 'epoch': 1.67}\n",
      "{'loss': 0.0081, 'grad_norm': 11.876587867736816, 'learning_rate': 2.742097340692424e-05, 'epoch': 1.68}\n",
      "{'loss': 0.002, 'grad_norm': 0.06819737702608109, 'learning_rate': 2.717009533366784e-05, 'epoch': 1.7}\n",
      "{'loss': 0.0021, 'grad_norm': 0.055341679602861404, 'learning_rate': 2.6919217260411438e-05, 'epoch': 1.71}\n",
      "{'loss': 0.1001, 'grad_norm': 0.03690572455525398, 'learning_rate': 2.6668339187155044e-05, 'epoch': 1.72}\n",
      "{'loss': 0.0058, 'grad_norm': 16.19825553894043, 'learning_rate': 2.6417461113898646e-05, 'epoch': 1.73}\n",
      "{'loss': 0.0765, 'grad_norm': 4.205867290496826, 'learning_rate': 2.616658304064225e-05, 'epoch': 1.74}\n",
      "{'loss': 0.0061, 'grad_norm': 0.07456699758768082, 'learning_rate': 2.591570496738585e-05, 'epoch': 1.76}\n",
      "{'loss': 0.0812, 'grad_norm': 13.616168975830078, 'learning_rate': 2.5664826894129456e-05, 'epoch': 1.77}\n",
      "{'loss': 0.0017, 'grad_norm': 0.03903529420495033, 'learning_rate': 2.541394882087306e-05, 'epoch': 1.78}\n",
      "{'loss': 0.0402, 'grad_norm': 0.07233784347772598, 'learning_rate': 2.5163070747616657e-05, 'epoch': 1.79}\n",
      "{'loss': 0.0633, 'grad_norm': 0.0595100037753582, 'learning_rate': 2.4912192674360263e-05, 'epoch': 1.81}\n",
      "{'loss': 0.0298, 'grad_norm': 0.2121727466583252, 'learning_rate': 2.4661314601103865e-05, 'epoch': 1.82}\n",
      "{'loss': 0.0367, 'grad_norm': 0.028998956084251404, 'learning_rate': 2.4410436527847468e-05, 'epoch': 1.83}\n",
      "{'loss': 0.0016, 'grad_norm': 0.028989989310503006, 'learning_rate': 2.415955845459107e-05, 'epoch': 1.84}\n",
      "{'loss': 0.0288, 'grad_norm': 0.06506282836198807, 'learning_rate': 2.3908680381334672e-05, 'epoch': 1.85}\n",
      "{'loss': 0.0275, 'grad_norm': 2.0001718997955322, 'learning_rate': 2.3657802308078275e-05, 'epoch': 1.87}\n",
      "{'loss': 0.0019, 'grad_norm': 0.1065504401922226, 'learning_rate': 2.3406924234821877e-05, 'epoch': 1.88}\n",
      "{'loss': 0.0019, 'grad_norm': 0.05746574327349663, 'learning_rate': 2.3156046161565483e-05, 'epoch': 1.89}\n",
      "{'loss': 0.0394, 'grad_norm': 0.03953578323125839, 'learning_rate': 2.2905168088309085e-05, 'epoch': 1.9}\n",
      "{'loss': 0.0012, 'grad_norm': 0.016573868691921234, 'learning_rate': 2.2654290015052684e-05, 'epoch': 1.91}\n",
      "{'loss': 0.028, 'grad_norm': 0.06843316555023193, 'learning_rate': 2.2403411941796286e-05, 'epoch': 1.93}\n",
      "{'loss': 0.0248, 'grad_norm': 0.1370343565940857, 'learning_rate': 2.2152533868539892e-05, 'epoch': 1.94}\n",
      "{'loss': 0.0491, 'grad_norm': 0.012583826668560505, 'learning_rate': 2.1901655795283494e-05, 'epoch': 1.95}\n",
      "{'loss': 0.022, 'grad_norm': 0.02761230245232582, 'learning_rate': 2.1650777722027096e-05, 'epoch': 1.96}\n",
      "{'loss': 0.0253, 'grad_norm': 0.05222922936081886, 'learning_rate': 2.13998996487707e-05, 'epoch': 1.97}\n",
      "{'loss': 0.036, 'grad_norm': 0.017290698364377022, 'learning_rate': 2.11490215755143e-05, 'epoch': 1.99}\n",
      "{'loss': 0.0106, 'grad_norm': 0.009547753259539604, 'learning_rate': 2.0898143502257903e-05, 'epoch': 2.0}\n",
      "{'loss': 0.0512, 'grad_norm': 0.055555347353219986, 'learning_rate': 2.0647265429001506e-05, 'epoch': 2.01}\n",
      "{'loss': 0.0015, 'grad_norm': 17.634246826171875, 'learning_rate': 2.0396387355745108e-05, 'epoch': 2.02}\n",
      "{'loss': 0.0014, 'grad_norm': 0.038750194013118744, 'learning_rate': 2.014550928248871e-05, 'epoch': 2.03}\n",
      "{'loss': 0.0105, 'grad_norm': 0.01563822478055954, 'learning_rate': 1.9894631209232316e-05, 'epoch': 2.05}\n",
      "{'loss': 0.0271, 'grad_norm': 0.021192843094468117, 'learning_rate': 1.9643753135975918e-05, 'epoch': 2.06}\n",
      "{'loss': 0.001, 'grad_norm': 0.46370092034339905, 'learning_rate': 1.939287506271952e-05, 'epoch': 2.07}\n",
      "{'loss': 0.001, 'grad_norm': 0.10904724150896072, 'learning_rate': 1.914199698946312e-05, 'epoch': 2.08}\n",
      "{'loss': 0.0009, 'grad_norm': 0.07356937974691391, 'learning_rate': 1.8891118916206725e-05, 'epoch': 2.09}\n",
      "{'loss': 0.0005, 'grad_norm': 0.01023924257606268, 'learning_rate': 1.8640240842950327e-05, 'epoch': 2.11}\n",
      "{'loss': 0.0007, 'grad_norm': 0.02235647663474083, 'learning_rate': 1.838936276969393e-05, 'epoch': 2.12}\n",
      "{'loss': 0.038, 'grad_norm': 0.009962521493434906, 'learning_rate': 1.8138484696437532e-05, 'epoch': 2.13}\n",
      "{'loss': 0.0338, 'grad_norm': 0.029460696503520012, 'learning_rate': 1.7887606623181134e-05, 'epoch': 2.14}\n",
      "{'loss': 0.0306, 'grad_norm': 0.010443827137351036, 'learning_rate': 1.763672854992474e-05, 'epoch': 2.15}\n",
      "{'loss': 0.0244, 'grad_norm': 0.00927360076457262, 'learning_rate': 1.738585047666834e-05, 'epoch': 2.17}\n",
      "{'loss': 0.0011, 'grad_norm': 0.04670431464910507, 'learning_rate': 1.713497240341194e-05, 'epoch': 2.18}\n",
      "{'loss': 0.0199, 'grad_norm': 0.00785592570900917, 'learning_rate': 1.6884094330155543e-05, 'epoch': 2.19}\n",
      "{'loss': 0.0011, 'grad_norm': 0.007767974399030209, 'learning_rate': 1.663321625689915e-05, 'epoch': 2.2}\n",
      "{'loss': 0.0005, 'grad_norm': 0.009279778227210045, 'learning_rate': 1.638233818364275e-05, 'epoch': 2.21}\n",
      "{'loss': 0.0011, 'grad_norm': 0.09801369905471802, 'learning_rate': 1.6131460110386354e-05, 'epoch': 2.23}\n",
      "{'loss': 0.0414, 'grad_norm': 0.006209223996847868, 'learning_rate': 1.5880582037129956e-05, 'epoch': 2.24}\n",
      "{'loss': 0.0352, 'grad_norm': 0.0060746315866708755, 'learning_rate': 1.562970396387356e-05, 'epoch': 2.25}\n",
      "{'loss': 0.0012, 'grad_norm': 0.1824430227279663, 'learning_rate': 1.537882589061716e-05, 'epoch': 2.26}\n",
      "{'loss': 0.001, 'grad_norm': 0.030619626864790916, 'learning_rate': 1.5127947817360763e-05, 'epoch': 2.27}\n",
      "{'loss': 0.0326, 'grad_norm': 0.2395872175693512, 'learning_rate': 1.4877069744104365e-05, 'epoch': 2.29}\n",
      "{'loss': 0.0012, 'grad_norm': 0.24429449439048767, 'learning_rate': 1.462619167084797e-05, 'epoch': 2.3}\n",
      "{'loss': 0.0026, 'grad_norm': 0.04016711562871933, 'learning_rate': 1.4375313597591572e-05, 'epoch': 2.31}\n",
      "{'loss': 0.0006, 'grad_norm': 0.004571159835904837, 'learning_rate': 1.4124435524335176e-05, 'epoch': 2.32}\n",
      "{'loss': 0.0006, 'grad_norm': 0.0043998900800943375, 'learning_rate': 1.3873557451078774e-05, 'epoch': 2.33}\n",
      "{'loss': 0.0497, 'grad_norm': 0.14852610230445862, 'learning_rate': 1.3622679377822378e-05, 'epoch': 2.35}\n",
      "{'loss': 0.0003, 'grad_norm': 0.0059734731912612915, 'learning_rate': 1.337180130456598e-05, 'epoch': 2.36}\n",
      "{'loss': 0.0556, 'grad_norm': 0.007849730551242828, 'learning_rate': 1.3120923231309585e-05, 'epoch': 2.37}\n",
      "{'loss': 0.0009, 'grad_norm': 0.004968143999576569, 'learning_rate': 1.2870045158053187e-05, 'epoch': 2.38}\n",
      "{'loss': 0.0009, 'grad_norm': 0.006718949880450964, 'learning_rate': 1.2619167084796791e-05, 'epoch': 2.39}\n",
      "{'loss': 0.0351, 'grad_norm': 0.004368409514427185, 'learning_rate': 1.2368289011540392e-05, 'epoch': 2.41}\n",
      "{'loss': 0.0548, 'grad_norm': 0.030002228915691376, 'learning_rate': 1.2117410938283996e-05, 'epoch': 2.42}\n",
      "{'loss': 0.0333, 'grad_norm': 0.13797011971473694, 'learning_rate': 1.1866532865027598e-05, 'epoch': 2.43}\n",
      "{'loss': 0.0012, 'grad_norm': 0.06663873791694641, 'learning_rate': 1.16156547917712e-05, 'epoch': 2.44}\n",
      "{'loss': 0.0313, 'grad_norm': 0.029895873740315437, 'learning_rate': 1.1364776718514803e-05, 'epoch': 2.45}\n",
      "{'loss': 0.0245, 'grad_norm': 41.18048858642578, 'learning_rate': 1.1113898645258405e-05, 'epoch': 2.47}\n",
      "{'loss': 0.0009, 'grad_norm': 0.045407310128211975, 'learning_rate': 1.0863020572002007e-05, 'epoch': 2.48}\n",
      "{'loss': 0.0006, 'grad_norm': 0.02534528635442257, 'learning_rate': 1.061214249874561e-05, 'epoch': 2.49}\n",
      "{'loss': 0.0274, 'grad_norm': 0.047420065850019455, 'learning_rate': 1.0361264425489213e-05, 'epoch': 2.5}\n",
      "{'loss': 0.05, 'grad_norm': 0.020486272871494293, 'learning_rate': 1.0110386352232816e-05, 'epoch': 2.52}\n",
      "{'loss': 0.0013, 'grad_norm': 0.07998082041740417, 'learning_rate': 9.859508278976418e-06, 'epoch': 2.53}\n",
      "{'loss': 0.0361, 'grad_norm': 0.05751706287264824, 'learning_rate': 9.60863020572002e-06, 'epoch': 2.54}\n",
      "{'loss': 0.0009, 'grad_norm': 0.014381383545696735, 'learning_rate': 9.357752132463624e-06, 'epoch': 2.55}\n",
      "{'loss': 0.0061, 'grad_norm': 0.0315815769135952, 'learning_rate': 9.106874059207225e-06, 'epoch': 2.56}\n",
      "{'loss': 0.0007, 'grad_norm': 0.018638810142874718, 'learning_rate': 8.855995985950829e-06, 'epoch': 2.58}\n",
      "{'loss': 0.0232, 'grad_norm': 16.233945846557617, 'learning_rate': 8.605117912694431e-06, 'epoch': 2.59}\n",
      "{'loss': 0.0005, 'grad_norm': 0.032215017825365067, 'learning_rate': 8.354239839438034e-06, 'epoch': 2.6}\n",
      "{'loss': 0.0008, 'grad_norm': 0.05346638336777687, 'learning_rate': 8.103361766181636e-06, 'epoch': 2.61}\n",
      "{'loss': 0.0334, 'grad_norm': 0.007234483025968075, 'learning_rate': 7.852483692925238e-06, 'epoch': 2.62}\n",
      "{'loss': 0.0008, 'grad_norm': 0.15212389826774597, 'learning_rate': 7.601605619668842e-06, 'epoch': 2.64}\n",
      "{'loss': 0.0007, 'grad_norm': 0.016197210177779198, 'learning_rate': 7.350727546412444e-06, 'epoch': 2.65}\n",
      "{'loss': 0.0007, 'grad_norm': 0.0053232647478580475, 'learning_rate': 7.099849473156047e-06, 'epoch': 2.66}\n",
      "{'loss': 0.0006, 'grad_norm': 0.012952836230397224, 'learning_rate': 6.848971399899649e-06, 'epoch': 2.67}\n",
      "{'loss': 0.0009, 'grad_norm': 0.0049816700629889965, 'learning_rate': 6.598093326643252e-06, 'epoch': 2.68}\n",
      "{'loss': 0.0006, 'grad_norm': 0.004359248094260693, 'learning_rate': 6.347215253386854e-06, 'epoch': 2.7}\n",
      "{'loss': 0.0005, 'grad_norm': 0.018606631085276604, 'learning_rate': 6.096337180130457e-06, 'epoch': 2.71}\n",
      "{'loss': 0.0608, 'grad_norm': 19.507095336914062, 'learning_rate': 5.845459106874059e-06, 'epoch': 2.72}\n",
      "{'loss': 0.0007, 'grad_norm': 0.0535135343670845, 'learning_rate': 5.594581033617662e-06, 'epoch': 2.73}\n",
      "{'loss': 0.0003, 'grad_norm': 0.020752090960741043, 'learning_rate': 5.3437029603612645e-06, 'epoch': 2.74}\n",
      "{'loss': 0.0004, 'grad_norm': 0.014426271431148052, 'learning_rate': 5.092824887104868e-06, 'epoch': 2.76}\n",
      "{'loss': 0.0004, 'grad_norm': 0.04442659765481949, 'learning_rate': 4.84194681384847e-06, 'epoch': 2.77}\n",
      "{'loss': 0.035, 'grad_norm': 0.015033580362796783, 'learning_rate': 4.591068740592072e-06, 'epoch': 2.78}\n",
      "{'loss': 0.0007, 'grad_norm': 0.026366762816905975, 'learning_rate': 4.3401906673356754e-06, 'epoch': 2.79}\n",
      "{'loss': 0.0004, 'grad_norm': 0.07390377670526505, 'learning_rate': 4.089312594079278e-06, 'epoch': 2.8}\n",
      "{'loss': 0.0005, 'grad_norm': 0.012918662279844284, 'learning_rate': 3.83843452082288e-06, 'epoch': 2.82}\n",
      "{'loss': 0.0004, 'grad_norm': 0.01403159461915493, 'learning_rate': 3.5875564475664828e-06, 'epoch': 2.83}\n",
      "{'loss': 0.0004, 'grad_norm': 0.004081912338733673, 'learning_rate': 3.336678374310086e-06, 'epoch': 2.84}\n",
      "{'loss': 0.0003, 'grad_norm': 0.005616971757262945, 'learning_rate': 3.0858003010536882e-06, 'epoch': 2.85}\n",
      "{'loss': 0.0004, 'grad_norm': 0.038389720022678375, 'learning_rate': 2.8349222277972905e-06, 'epoch': 2.86}\n",
      "{'loss': 0.0003, 'grad_norm': 0.004650597460567951, 'learning_rate': 2.5840441545408932e-06, 'epoch': 2.88}\n",
      "{'loss': 0.0004, 'grad_norm': 0.04761618748307228, 'learning_rate': 2.333166081284496e-06, 'epoch': 2.89}\n",
      "{'loss': 0.0003, 'grad_norm': 0.03554930165410042, 'learning_rate': 2.0822880080280983e-06, 'epoch': 2.9}\n",
      "{'loss': 0.0004, 'grad_norm': 0.0034107358660548925, 'learning_rate': 1.831409934771701e-06, 'epoch': 2.91}\n",
      "{'loss': 0.0006, 'grad_norm': 3.6559970378875732, 'learning_rate': 1.5805318615153035e-06, 'epoch': 2.92}\n",
      "{'loss': 0.0005, 'grad_norm': 0.0040885936468839645, 'learning_rate': 1.3296537882589062e-06, 'epoch': 2.94}\n",
      "{'loss': 0.0005, 'grad_norm': 0.023391133174300194, 'learning_rate': 1.078775715002509e-06, 'epoch': 2.95}\n",
      "{'loss': 0.0005, 'grad_norm': 0.01004484761506319, 'learning_rate': 8.278976417461115e-07, 'epoch': 2.96}\n",
      "{'loss': 0.0556, 'grad_norm': 0.0038536146748811007, 'learning_rate': 5.770195684897141e-07, 'epoch': 2.97}\n",
      "{'loss': 0.0005, 'grad_norm': 0.004710442386567593, 'learning_rate': 3.261414952333166e-07, 'epoch': 2.98}\n",
      "{'loss': 0.0005, 'grad_norm': 0.013558181002736092, 'learning_rate': 7.526342197691922e-08, 'epoch': 3.0}\n",
      "{'train_runtime': 2337.679, 'train_samples_per_second': 17.045, 'train_steps_per_second': 1.066, 'train_loss': 0.03562983859471814, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "963b27ce99a34790a0231a585c08a159",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/416 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics: {'eval_loss': 0.013381868600845337, 'eval_runtime': 59.9438, 'eval_samples_per_second': 55.402, 'eval_steps_per_second': 6.94, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "637c66c68e624db88cbecbb5aab37dea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/416 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.99      0.97      0.98       144\n",
      "    Positive       1.00      1.00      1.00      3177\n",
      "\n",
      "    accuracy                           1.00      3321\n",
      "   macro avg       0.99      0.99      0.99      3321\n",
      "weighted avg       1.00      1.00      1.00      3321\n",
      "\n",
      "Model is on device: cuda:0\n",
      "Processed 1_bai_hotel, results saved to C:\\Users\\andyb\\Desktop\\Coding Files\\PointView\\datasets\\sentiment_results\\1_bai_hotel\\1_bai_hotel_sentiment_analysis.csv\n",
      "Processed 2_dusit_thani_mactan, results saved to C:\\Users\\andyb\\Desktop\\Coding Files\\PointView\\datasets\\sentiment_results\\2_dusit_thani_mactan\\2_dusit_thani_mactan_sentiment_analysis.csv\n",
      "Processed 3_fairfield_by_marriott_cebu, results saved to C:\\Users\\andyb\\Desktop\\Coding Files\\PointView\\datasets\\sentiment_results\\3_fairfield_by_marriott_cebu\\3_fairfield_by_marriott_cebu_sentiment_analysis.csv\n",
      "Processed 4_jpark_island_resort_and_waterpark, results saved to C:\\Users\\andyb\\Desktop\\Coding Files\\PointView\\datasets\\sentiment_results\\4_jpark_island_resort_and_waterpark\\4_jpark_island_resort_and_waterpark_sentiment_analysis.csv\n",
      "Processed 5_seda_ayala_center_cebu, results saved to C:\\Users\\andyb\\Desktop\\Coding Files\\PointView\\datasets\\sentiment_results\\5_seda_ayala_center_cebu\\5_seda_ayala_center_cebu_sentiment_analysis.csv\n",
      "Processed 6_waterfront_hotel_and_casino, results saved to C:\\Users\\andyb\\Desktop\\Coding Files\\PointView\\datasets\\sentiment_results\\6_waterfront_hotel_and_casino\\6_waterfront_hotel_and_casino_sentiment_analysis.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch.backends.cudnn as cudnn\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Download necessary nltk data\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Ensure deterministic behavior\n",
    "cudnn.deterministic = True\n",
    "cudnn.benchmark = False\n",
    "\n",
    "# Check if GPU is available and set the device accordingly\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Preprocessing Functions for BERT\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean the input text by:\n",
    "    1. Removing special characters and punctuation.\n",
    "    2. Converting text to lowercase.\n",
    "    3. Handling contractions (optional for your use case).\n",
    "    \"\"\"\n",
    "    # Remove special characters, numbers, and punctuation\n",
    "    text = re.sub(f\"[{string.punctuation}]\", \" \", text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Tokenization and lemmatization\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "# Apply to labeled dataset\n",
    "labeled_data_path = r\"C:\\Users\\andyb\\Desktop\\Coding Files\\pointview\\datasets\\labeled_dataset.csv\"\n",
    "df = pd.read_csv(labeled_data_path)\n",
    "\n",
    "# Clean the 'Opinion' column\n",
    "df['Opinion'] = df['Opinion'].apply(clean_text)\n",
    "\n",
    "# Convert the sentiment labels to integers\n",
    "label_mapping = {'Positive': 1, 'Negative': 0}\n",
    "df['Sentiment'] = df['Sentiment'].map(label_mapping)\n",
    "\n",
    "# Handle NaN values in the Sentiment column\n",
    "df = df.dropna(subset=['Sentiment'])\n",
    "\n",
    "# Ensure labels are of type int\n",
    "df['Sentiment'] = df['Sentiment'].astype(int)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenize the dataset\n",
    "train_encodings = tokenizer(train_df['Opinion'].tolist(), truncation=True, padding=True, max_length=512)\n",
    "train_labels = train_df['Sentiment'].tolist()\n",
    "\n",
    "test_encodings = tokenizer(test_df['Opinion'].tolist(), truncation=True, padding=True, max_length=512)\n",
    "test_labels = test_df['Sentiment'].tolist()\n",
    "\n",
    "# Create custom Dataset class\n",
    "class ReviewDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx], dtype=torch.long) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Create datasets for training and testing\n",
    "train_dataset = ReviewDataset(train_encodings, train_labels)\n",
    "test_dataset = ReviewDataset(test_encodings, test_labels)\n",
    "\n",
    "# Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Trainer for training and evaluation\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=test_dataset            # evaluation dataset\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "metrics = trainer.evaluate()\n",
    "print(f\"Evaluation metrics: {metrics}\")\n",
    "\n",
    "# Predict and evaluate on the test set\n",
    "predictions = trainer.predict(test_dataset)\n",
    "preds = np.argmax(predictions.predictions, axis=1)\n",
    "\n",
    "# Classification report for precision, recall, and F1-score\n",
    "print(classification_report(test_labels, preds, target_names=['Negative', 'Positive']))\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"./fine_tuned_model\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_model\")\n",
    "\n",
    "# Load the fine-tuned model\n",
    "model = BertForSequenceClassification.from_pretrained('./fine_tuned_model')\n",
    "tokenizer = BertTokenizer.from_pretrained('./fine_tuned_model')\n",
    "\n",
    "# Move the model to the GPU\n",
    "model.to(device)\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Base directory containing the hotel reviews\n",
    "base_dir = r\"C:\\Users\\andyb\\Desktop\\Coding Files\\PointView\\datasets\\hotel_aspect_based_dataset\"\n",
    "\n",
    "# Output directory to save the sentiment results\n",
    "output_dir = r\"C:\\Users\\andyb\\Desktop\\Coding Files\\PointView\\datasets\\sentiment_results\"\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Function to extract aspects from review content based on the KPIs\n",
    "def extract_aspects(review, aspects_list):\n",
    "    return [aspect for aspect in aspects_list if aspect.lower() in review.lower()]\n",
    "\n",
    "# Check if the model is on the GPU\n",
    "print(f\"Model is on device: {next(model.parameters()).device}\")\n",
    "\n",
    "# When you're processing inputs:\n",
    "def predict_sentiment(review, aspect):\n",
    "    input_text = f\"{aspect}: {review}\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)  # Move inputs to GPU\n",
    "    outputs = model(**inputs)\n",
    "    sentiment = torch.argmax(outputs.logits, dim=1).item()\n",
    "    return \"Positive\" if sentiment == 1 else \"Negative\"\n",
    "\n",
    "# Loop through each hotel directory and process the combined data\n",
    "for hotel_dir in os.listdir(base_dir):\n",
    "    hotel_path = os.path.join(base_dir, hotel_dir)\n",
    "    \n",
    "    if os.path.isdir(hotel_path):  # Check if it's a directory\n",
    "        combined_df = pd.DataFrame()  # Initialize an empty DataFrame to combine all files\n",
    "\n",
    "        # Combine all CSV files within the hotel directory\n",
    "        for csv_file in os.listdir(hotel_path):\n",
    "            if csv_file.endswith('.csv'):\n",
    "                file_path = os.path.join(hotel_path, csv_file)\n",
    "                temp_df = pd.read_csv(file_path)\n",
    "                combined_df = pd.concat([combined_df, temp_df], ignore_index=True)\n",
    "\n",
    "        # Perform the sentiment analysis on the combined DataFrame\n",
    "        combined_df['Aspects'] = combined_df['Review Content'].apply(lambda x: extract_aspects(x, specific_kpis))\n",
    "        combined_df['Aspects'] = combined_df['Aspects'].apply(lambda x: x if x else [])\n",
    "\n",
    "        combined_df['Sentiment_Results'] = combined_df.apply(\n",
    "            lambda row: {aspect: predict_sentiment(row['Review Content'], aspect) for aspect in row['Aspects']},\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        # Initialize dictionary to track positive/negative counts for each KPI\n",
    "        aspect_sentiments = {aspect: {'positive': 0, 'negative': 0} for aspect in specific_kpis}\n",
    "\n",
    "        # Count the positive and negative sentiments for each aspect\n",
    "        for index, row in combined_df.iterrows():\n",
    "            for aspect, sentiment in row['Sentiment_Results'].items():\n",
    "                if sentiment == \"Positive\":\n",
    "                    aspect_sentiments[aspect]['positive'] += 1\n",
    "                else:\n",
    "                    aspect_sentiments[aspect]['negative'] += 1\n",
    "\n",
    "        # Calculate sentiment percentages for each aspect\n",
    "        total_reviews = len(combined_df)\n",
    "        for aspect, counts in aspect_sentiments.items():\n",
    "            counts['positive_percent'] = (counts['positive'] / total_reviews) * 100\n",
    "            counts['negative_percent'] = (counts['negative'] / total_reviews) * 100\n",
    "\n",
    "        # Create a folder for the hotel in the output directory\n",
    "        hotel_output_dir = os.path.join(output_dir, hotel_dir)\n",
    "        if not os.path.exists(hotel_output_dir):\n",
    "            os.makedirs(hotel_output_dir)\n",
    "\n",
    "        # Save the sentiment analysis results to a CSV file\n",
    "        output_file_path = os.path.join(hotel_output_dir, f\"{hotel_dir}_sentiment_analysis.csv\")\n",
    "        output_df = pd.DataFrame(aspect_sentiments).T\n",
    "        output_df.to_csv(output_file_path)\n",
    "\n",
    "        print(f\"Processed {hotel_dir}, results saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Analysis for 1_bai_hotel:\n",
      "                      positive  negative  positive_percent  negative_percent\n",
      "food                    1111.0      46.0         27.506809          1.138896\n",
      "staff                   1679.0      82.0         41.569695          2.030205\n",
      "comfort & facilities       1.0       0.0          0.024759          0.000000\n",
      "value for money           53.0       1.0          1.312206          0.024759\n",
      "\n",
      "==================================================\n",
      "\n",
      "Sentiment Analysis for 2_dusit_thani_mactan:\n",
      "                      positive  negative  positive_percent  negative_percent\n",
      "food                     536.0      55.0         28.389831          2.913136\n",
      "staff                    882.0      63.0         46.716102          3.336864\n",
      "comfort & facilities       0.0       0.0          0.000000          0.000000\n",
      "value for money           12.0       1.0          0.635593          0.052966\n",
      "\n",
      "==================================================\n",
      "\n",
      "Sentiment Analysis for 3_fairfield_by_marriott_cebu:\n",
      "                      positive  negative  positive_percent  negative_percent\n",
      "food                      27.0       0.0         25.233645          0.000000\n",
      "staff                     51.0       2.0         47.663551          1.869159\n",
      "comfort & facilities       0.0       0.0          0.000000          0.000000\n",
      "value for money            3.0       0.0          2.803738          0.000000\n",
      "\n",
      "==================================================\n",
      "\n",
      "Sentiment Analysis for 4_jpark_island_resort_and_waterpark:\n",
      "                      positive  negative  positive_percent  negative_percent\n",
      "food                     114.0       8.0         20.540541          1.441441\n",
      "staff                    157.0      15.0         28.288288          2.702703\n",
      "comfort & facilities       0.0       0.0          0.000000          0.000000\n",
      "value for money            5.0       0.0          0.900901          0.000000\n",
      "\n",
      "==================================================\n",
      "\n",
      "Sentiment Analysis for 5_seda_ayala_center_cebu:\n",
      "                      positive  negative  positive_percent  negative_percent\n",
      "food                     262.0      25.0         13.326551          1.271617\n",
      "staff                    729.0      52.0         37.080366          2.644964\n",
      "comfort & facilities       0.0       0.0          0.000000          0.000000\n",
      "value for money           35.0       2.0          1.780264          0.101729\n",
      "\n",
      "==================================================\n",
      "\n",
      "Sentiment Analysis for 6_waterfront_hotel_and_casino:\n",
      "                      positive  negative  positive_percent  negative_percent\n",
      "food                     185.0      41.0         10.966212          2.430350\n",
      "staff                    433.0      51.0         25.666864          3.023118\n",
      "comfort & facilities       0.0       0.0          0.000000          0.000000\n",
      "value for money           23.0       2.0          1.363367          0.118554\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the output directory where sentiment results are saved\n",
    "output_dir = r\"C:\\Users\\andyb\\Desktop\\Coding Files\\PointView\\datasets\\sentiment_results\"\n",
    "\n",
    "# Function to load and display sentiment results from all hotels\n",
    "def load_and_display_sentiment_results(output_dir):\n",
    "    for hotel_dir in os.listdir(output_dir):\n",
    "        hotel_path = os.path.join(output_dir, hotel_dir)\n",
    "        \n",
    "        if os.path.isdir(hotel_path):  # Check if it's a directory\n",
    "            for csv_file in os.listdir(hotel_path):\n",
    "                if csv_file.endswith('_sentiment_analysis.csv'):\n",
    "                    file_path = os.path.join(hotel_path, csv_file)\n",
    "                    \n",
    "                    # Load the CSV file into a DataFrame\n",
    "                    df = pd.read_csv(file_path, index_col=0)\n",
    "                    \n",
    "                    # Display the DataFrame\n",
    "                    print(f\"Sentiment Analysis for {hotel_dir}:\")\n",
    "                    print(df)\n",
    "                    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Call the function to display all sentiment results\n",
    "load_and_display_sentiment_results(output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KPI Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Additional KPIs identified for 1_bai_hotel: ['experience', 'expectations', 'extra', 'extremely', 'facilities', 'family', 'exceptional', 'worth', 'hotel', 'fantastic']\n",
      "Additional KPIs identified for 2_dusit_thani_mactan: ['far', 'feel', 'felt', 'filipino', 'zee', 'fantastic', 'fine', 'hotel']\n",
      "Additional KPIs identified for 3_fairfield_by_marriott_cebu: ['facilities', 'fairfield', 'face', 'extremely', 'extra', 'yes', 'fabulous', 'nice', 'room', 'great']\n",
      "Additional KPIs identified for 4_jpark_island_resort_and_waterpark: ['facility', 'fact', 'facilities', 'families', 'family', 'young', 'fantastic', 'extra', 'felt', 'far']\n",
      "Additional KPIs identified for 5_seda_ayala_center_cebu: ['extra', 'facilities', 'experience', 'family', 'fantastic', 'worth', 'far', 'hotel', 'fast', 'location']\n",
      "Additional KPIs identified for 6_waterfront_hotel_and_casino: ['excellent', 'event', 'exceptional', 'especially', 'expensive', 'experience', 'years', 'extra', 'enjoyed', 'hotel']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "\n",
    "# Define your specific KPIs\n",
    "specific_kpis = ['food', 'staff', 'comfort & facilities', 'value for money']\n",
    "\n",
    "# Function to extract keywords using TF-IDF\n",
    "def extract_keywords_tfidf(reviews, top_n=10):\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=10000, min_df=0.01, stop_words='english')\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(reviews)\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "    tfidf_scores = tfidf_matrix.toarray()\n",
    "    \n",
    "    top_keywords = []\n",
    "    for doc in tfidf_scores:\n",
    "        sorted_indices = doc.argsort()[-top_n:]\n",
    "        top_keywords.extend([feature_names[i] for i in sorted_indices])\n",
    "    \n",
    "    return Counter(top_keywords).most_common(top_n)\n",
    "\n",
    "# Example usage with a single hotel's reviews\n",
    "def get_additional_kpis(reviews):\n",
    "    keywords = extract_keywords_tfidf(reviews, top_n=10)\n",
    "    additional_kpis = [keyword for keyword, _ in keywords if keyword not in specific_kpis]\n",
    "    return additional_kpis\n",
    "\n",
    "# Directory paths\n",
    "base_dir = r\"C:\\Users\\andyb\\Desktop\\Coding Files\\PointView\\datasets\\hotel_aspect_based_dataset\"\n",
    "\n",
    "# Process each hotel's reviews\n",
    "for hotel_dir in os.listdir(base_dir):\n",
    "    hotel_path = os.path.join(base_dir, hotel_dir)\n",
    "    \n",
    "    if os.path.isdir(hotel_path):\n",
    "        combined_df = pd.DataFrame()\n",
    "\n",
    "        # Combine all CSV files within the hotel directory\n",
    "        for csv_file in os.listdir(hotel_path):\n",
    "            if csv_file.endswith('.csv'):\n",
    "                file_path = os.path.join(hotel_path, csv_file)\n",
    "                temp_df = pd.read_csv(file_path)\n",
    "                combined_df = pd.concat([combined_df, temp_df], ignore_index=True)\n",
    "\n",
    "        # Extract additional KPIs\n",
    "        reviews = combined_df['Review Content'].tolist()\n",
    "        additional_kpis = get_additional_kpis(reviews)\n",
    "        \n",
    "        print(f\"Additional KPIs identified for {hotel_dir}: {additional_kpis}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\andyb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Additional KPIs identified using LDA for 1_bai_hotel: ['birthday', 'hotel', 'breakfast', 'thank', 'us', 'even', 'good', 'really', 'customer', 'service', 'room', 'great', 'bai', 'buffet']\n",
      "Additional KPIs identified using LDA for 2_dusit_thani_mactan: ['pool', 'hotel', 'us', 'good', '-', 'service', 'room', 'great']\n",
      "Additional KPIs identified using LDA for 3_fairfield_by_marriott_cebu: ['clean', 'time', 'hotel', 'breakfast', 'incredible', 'bed', 'amazing', 'toilet', 'rooms', 'beautiful', 'nice', 'good', 'room', 'great', 'would', 'many']\n",
      "Additional KPIs identified using LDA for 4_jpark_island_resort_and_waterpark: ['pool', 'hotel', 'water', '•', 'resort', 'good', 'kids', '-', 'room', 'great', 'place']\n",
      "Additional KPIs identified using LDA for 5_seda_ayala_center_cebu: ['mall', 'hotel', 'breakfast', 'ayala', 'seda', 'check', 'stay', 'good', '-', 'room', 'location']\n",
      "Additional KPIs identified using LDA for 6_waterfront_hotel_and_casino: ['hotel', 'breakfast', 'check', 'stay', 'good', 'nice', 'old', 'room', 'great']\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Function to preprocess reviews for LDA\n",
    "def preprocess_for_lda(reviews):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    texts = [[word for word in review.lower().split() if word not in stop_words] for review in reviews]\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    return corpus, dictionary\n",
    "\n",
    "# Function to extract topics using LDA\n",
    "def extract_topics_lda(corpus, dictionary, num_topics=5):\n",
    "    lda_model = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=15)\n",
    "    topics = lda_model.print_topics(num_words=5)\n",
    "    topic_keywords = []\n",
    "    for topic in topics:\n",
    "        words = topic[1].split(' + ')\n",
    "        keywords = [word.split('*')[-1].replace('\"', '').strip() for word in words]\n",
    "        topic_keywords.extend(keywords)\n",
    "    return list(set(topic_keywords))\n",
    "\n",
    "# Example usage with a single hotel's reviews\n",
    "def get_additional_kpis_lda(reviews):\n",
    "    corpus, dictionary = preprocess_for_lda(reviews)\n",
    "    topics = extract_topics_lda(corpus, dictionary, num_topics=5)\n",
    "    additional_kpis = [topic for topic in topics if topic not in specific_kpis]\n",
    "    return additional_kpis\n",
    "\n",
    "# Process each hotel's reviews\n",
    "for hotel_dir in os.listdir(base_dir):\n",
    "    hotel_path = os.path.join(base_dir, hotel_dir)\n",
    "    \n",
    "    if os.path.isdir(hotel_path):\n",
    "        combined_df = pd.DataFrame()\n",
    "\n",
    "        # Combine all CSV files within the hotel directory\n",
    "        for csv_file in os.listdir(hotel_path):\n",
    "            if csv_file.endswith('.csv'):\n",
    "                file_path = os.path.join(hotel_path, csv_file)\n",
    "                temp_df = pd.read_csv(file_path)\n",
    "                combined_df = pd.concat([combined_df, temp_df], ignore_index=True)\n",
    "\n",
    "        # Extract additional KPIs using LDA\n",
    "        reviews = combined_df['Review Content'].tolist()\n",
    "        additional_kpis_lda = get_additional_kpis_lda(reviews)\n",
    "        \n",
    "        print(f\"Additional KPIs identified using LDA for {hotel_dir}: {additional_kpis_lda}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All KPIs for 1_bai_hotel: ['birthday', 'like', 'hotel', 'food', 'even', 'facilities', 'really', 'room', 'great', 'cant', 'exceptional', 'staff', 'value for money', 'breakfast', 'fantastic', 'extra', 'extremely', 'customer', 'would', 'expectations', 'also', 'comfort & facilities', 'us', 'experience', 'good', 'worth', 'service', 'family', 'bai']\n",
      "All KPIs for 2_dusit_thani_mactan: ['hotel', 'food', 'room', 'great', 'staff', 'filipino', 'value for money', 'stay', 'fantastic', 'pool', 'comfort & facilities', 'feel', 'zee', 'felt', 'view', 'far', 'fine', 'dusit', 'good', 'service']\n",
      "All KPIs for 3_fairfield_by_marriott_cebu: ['clean', 'like', 'hotel', 'bed', 'yes', 'food', 'facilities', 'really', 'room', 'great', 'fairfield', 'everything', 'staff', 'value for money', 'breakfast', 'extra', 'best', 'extremely', 'rooms', 'nice', 'comfort & facilities', 'us', 'fabulous', 'experience', 'face', 'good']\n",
      "All KPIs for 4_jpark_island_resort_and_waterpark: ['young', 'hotel', 'food', 'facilities', 'families', 'room', 'great', 'many', 'staff', 'value for money', 'fact', 'fantastic', 'extra', '-', 'pool', 'comfort & facilities', 'water', 'us', 'resort', 'facility', 'place', 'felt', 'far', 'get', 'good', 'kids', 'service', 'family']\n",
      "All KPIs for 5_seda_ayala_center_cebu: ['hotel', 'ayala', 'seda', 'food', 'facilities', 'room', 'great', 'staff', 'value for money', 'breakfast', 'stay', 'fantastic', 'extra', 'comfort & facilities', 'fast', 'far', 'experience', 'check', 'good', 'worth', 'family', 'location']\n",
      "All KPIs for 6_waterfront_hotel_and_casino: ['excellent', 'years', 'hotel', 'food', 'expensive', 'room', 'great', 'exceptional', 'staff', 'value for money', 'breakfast', 'stay', 'event', 'extra', 'nice', 'especially', 'pool', 'comfort & facilities', 'experience', 'enjoyed', 'check', '.', 'good']\n"
     ]
    }
   ],
   "source": [
    "def combine_kpis(predefined_kpis, additional_kpis):\n",
    "    return list(set(predefined_kpis + additional_kpis))\n",
    "\n",
    "# Combine TF-IDF and LDA results with predefined KPIs\n",
    "for hotel_dir in os.listdir(base_dir):\n",
    "    hotel_path = os.path.join(base_dir, hotel_dir)\n",
    "    \n",
    "    if os.path.isdir(hotel_path):\n",
    "        combined_df = pd.DataFrame()\n",
    "\n",
    "        for csv_file in os.listdir(hotel_path):\n",
    "            if csv_file.endswith('.csv'):\n",
    "                file_path = os.path.join(hotel_path, csv_file)\n",
    "                temp_df = pd.read_csv(file_path)\n",
    "                combined_df = pd.concat([combined_df, temp_df], ignore_index=True)\n",
    "\n",
    "        reviews = combined_df['Review Content'].tolist()\n",
    "\n",
    "        # Extract additional KPIs using both TF-IDF and LDA\n",
    "        additional_kpis_tfidf = get_additional_kpis(reviews)\n",
    "        additional_kpis_lda = get_additional_kpis_lda(reviews)\n",
    "        additional_kpis = list(set(additional_kpis_tfidf + additional_kpis_lda))\n",
    "\n",
    "        # Combine with predefined KPIs\n",
    "        all_kpis = combine_kpis(specific_kpis, additional_kpis)\n",
    "\n",
    "        print(f\"All KPIs for {hotel_dir}: {all_kpis}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
