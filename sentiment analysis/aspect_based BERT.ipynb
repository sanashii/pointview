{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the data to be used for model fine tuning for consistent results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aspect_Sentiment column before exploding:\n",
      "0        {'Staff': 'Positive', 'Location': 'Positive'}\n",
      "1    {'Food': 'Positive', 'Comfort & Facilities': '...\n",
      "2         {'Location': 'Positive', 'Food': 'Positive'}\n",
      "3    {'Comfort & Facilities': 'Positive', 'Food': '...\n",
      "4    {'Comfort & Facilities': 'Positive', 'Food': '...\n",
      "Name: Aspect_Sentiment, dtype: object\n",
      "Aspect_Sentiment column after conversion:\n",
      "0        {'Staff': 'Positive', 'Location': 'Positive'}\n",
      "1    {'Food': 'Positive', 'Comfort & Facilities': '...\n",
      "2         {'Location': 'Positive', 'Food': 'Positive'}\n",
      "3    {'Comfort & Facilities': 'Positive', 'Food': '...\n",
      "4    {'Comfort & Facilities': 'Positive', 'Food': '...\n",
      "Name: Aspect_Sentiment, dtype: object\n",
      "Exploded DataFrame:\n",
      "                                             Opinion                Aspect  \\\n",
      "0  We stayed for a week and could not fault it at...                 Staff   \n",
      "1  We stayed for a week and could not fault it at...              Location   \n",
      "2  This resort is beautiful. The rooms are fabulo...                  Food   \n",
      "3  This resort is beautiful. The rooms are fabulo...  Comfort & Facilities   \n",
      "4  i never fail to visit Shangrila Boracay eveyti...              Location   \n",
      "\n",
      "  Sentiment  \n",
      "0  Positive  \n",
      "1  Positive  \n",
      "2  Positive  \n",
      "3  Positive  \n",
      "4  Positive  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "import ast\n",
    "import glob\n",
    "\n",
    "# Load the dataset\n",
    "aspect_path = r\"C:\\Users\\andyb\\Desktop\\Coding Files\\PointView\\datasets\\aspect_based_dataset\\*.csv\"\n",
    "files = glob.glob(aspect_path)\n",
    "\n",
    "# Concatenate all CSV files into a single DataFrame\n",
    "dataframes = [pd.read_csv(f) for f in files]\n",
    "df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Ensure all 'Classification' values are strings and handle missing values\n",
    "df['Classification'] = df['Classification'].fillna('').astype(str)\n",
    "\n",
    "def get_sentiment(opinion):\n",
    "    analysis = TextBlob(opinion)\n",
    "    # Determine the polarity (-1 is very negative, +1 is very positive)\n",
    "    if analysis.sentiment.polarity > 0:\n",
    "        return 'Positive'\n",
    "    elif analysis.sentiment.polarity < 0:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "def label_aspects(row):\n",
    "    sentiments = {}\n",
    "    aspects = row['Classification'].split(':')\n",
    "    for aspect in aspects:\n",
    "        sentiment = get_sentiment(row['Opinion'])\n",
    "        sentiments[aspect] = sentiment\n",
    "    return sentiments\n",
    "\n",
    "# Apply the labeling function to each row\n",
    "df['Aspect_Sentiment'] = df.apply(label_aspects, axis=1)\n",
    "\n",
    "# Check the first few entries in the 'Aspect_Sentiment' column\n",
    "print(\"Aspect_Sentiment column before exploding:\")\n",
    "print(df['Aspect_Sentiment'].head())\n",
    "\n",
    "# Convert 'Aspect_Sentiment' to actual dictionaries if they are strings\n",
    "df['Aspect_Sentiment'] = df['Aspect_Sentiment'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "# Check again after conversion\n",
    "print(\"Aspect_Sentiment column after conversion:\")\n",
    "print(df['Aspect_Sentiment'].head())\n",
    "\n",
    "# Manually explode the dictionary into separate rows\n",
    "rows = []\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    for aspect, sentiment in row['Aspect_Sentiment'].items():\n",
    "        rows.append({\n",
    "            'Opinion': row['Opinion'],\n",
    "            'Aspect': aspect,\n",
    "            'Sentiment': sentiment\n",
    "        })\n",
    "\n",
    "exploded_df = pd.DataFrame(rows)\n",
    "\n",
    "# Check the final output\n",
    "print(\"Exploded DataFrame:\")\n",
    "print(exploded_df.head())\n",
    "\n",
    "# Save the labeled dataset\n",
    "exploded_df.to_csv(r'C:\\Users\\andyb\\Desktop\\Coding Files\\PointView\\datasets\\labeled_dataset.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Ensure deterministic behavior\n",
    "cudnn.deterministic = True\n",
    "cudnn.benchmark = False\n",
    "\n",
    "# Check if GPU is available and set the device accordingly\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Move the model to the GPU\n",
    "model.to(device)\n",
    "\n",
    "# Set model to evaluation mode to disable dropout\n",
    "model.eval()\n",
    "\n",
    "# Define your specific KPIs\n",
    "specific_kpis = ['food', 'staff', 'comfort', 'facilities', 'value for money']\n",
    "\n",
    "# Load the labeled dataset (for fine-tuning)\n",
    "labeled_data_path = r\"C:\\Users\\andyb\\Desktop\\Coding Files\\pointview\\datasets\\labeled_dataset.csv\"\n",
    "df = pd.read_csv(labeled_data_path)\n",
    "\n",
    "# Convert the sentiment labels to integers\n",
    "label_mapping = {'Positive': 1, 'Negative': 0}\n",
    "df['label'] = df['label'].map(label_mapping)\n",
    "\n",
    "# Identify rows with NaN values after mapping\n",
    "nan_rows = df[df['label'].isna()]\n",
    "print(\"Rows with NaN values in 'label':\")\n",
    "print(nan_rows)\n",
    "\n",
    "# Handle NaN values\n",
    "df = df.dropna(subset=['label'])  # Option 1: Drop rows with NaN values\n",
    "\n",
    "# Ensure the labels are of type int\n",
    "df['label'] = df['label'].astype(int)\n",
    "\n",
    "# Inspect the label values and their data types\n",
    "print(\"Unique values in label column:\", df['label'].unique())\n",
    "print(\"Data type of label column:\", df['label'].dtype)\n",
    "\n",
    "# Tokenize the dataset\n",
    "train_encodings = tokenizer(df['text'].tolist(), truncation=True, padding=True, max_length=512)\n",
    "train_labels = df['label'].tolist()  # These should now be integers\n",
    "\n",
    "# Create a custom Dataset class\n",
    "class ReviewDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx], dtype=torch.long) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = ReviewDataset(train_encodings, train_labels)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset          # training dataset\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"./fine_tuned_model\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_model\")\n",
    "\n",
    "\n",
    "# Load the fine-tuned model\n",
    "model = BertForSequenceClassification.from_pretrained('./fine_tuned_model')\n",
    "tokenizer = BertTokenizer.from_pretrained('./fine_tuned_model')\n",
    "\n",
    "# Move the model to the GPU\n",
    "model.to(device)\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Base directory containing the hotel reviews\n",
    "base_dir = r\"C:\\Users\\andyb\\Desktop\\Coding Files\\PointView\\datasets\\hotel_aspect_based_dataset\"\n",
    "\n",
    "# Output directory to save the sentiment results\n",
    "output_dir = r\"C:\\Users\\andyb\\Desktop\\Coding Files\\PointView\\datasets\\sentiment_results\"\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Function to extract aspects from review content based on the KPIs\n",
    "def extract_aspects(review, aspects_list):\n",
    "    return [aspect for aspect in aspects_list if aspect.lower() in review.lower()]\n",
    "\n",
    "# Check if the model is on the GPU\n",
    "print(f\"Model is on device: {next(model.parameters()).device}\")\n",
    "\n",
    "# When you're processing inputs:\n",
    "def predict_sentiment(review, aspect):\n",
    "    input_text = f\"{aspect}: {review}\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)  # Move inputs to GPU\n",
    "    print(f\"Inputs are on device: {inputs['input_ids'].device}\")  # Check if the inputs are on GPU\n",
    "    outputs = model(**inputs)\n",
    "    sentiment = torch.argmax(outputs.logits, dim=1).item()\n",
    "    return \"Positive\" if sentiment == 1 else \"Negative\"\n",
    "\n",
    "# Loop through each hotel directory and process the combined data\n",
    "for hotel_dir in os.listdir(base_dir):\n",
    "    hotel_path = os.path.join(base_dir, hotel_dir)\n",
    "    \n",
    "    if os.path.isdir(hotel_path):  # Check if it's a directory\n",
    "        combined_df = pd.DataFrame()  # Initialize an empty DataFrame to combine all files\n",
    "\n",
    "        # Combine all CSV files within the hotel directory\n",
    "        for csv_file in os.listdir(hotel_path):\n",
    "            if csv_file.endswith('.csv'):\n",
    "                file_path = os.path.join(hotel_path, csv_file)\n",
    "                temp_df = pd.read_csv(file_path)\n",
    "                combined_df = pd.concat([combined_df, temp_df], ignore_index=True)\n",
    "\n",
    "        # Perform the sentiment analysis on the combined DataFrame\n",
    "        combined_df['Aspects'] = combined_df['Review Content'].apply(lambda x: extract_aspects(x, specific_kpis))\n",
    "        combined_df['Aspects'] = combined_df['Aspects'].apply(lambda x: x if x else [])\n",
    "\n",
    "        combined_df['Sentiment_Results'] = combined_df.apply(\n",
    "            lambda row: {aspect: predict_sentiment(row['Review Content'], aspect) for aspect in row['Aspects']},\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        # Initialize dictionary to track positive/negative counts for each KPI\n",
    "        aspect_sentiments = {aspect: {'positive': 0, 'negative': 0} for aspect in specific_kpis}\n",
    "\n",
    "        # Count the positive and negative sentiments for each aspect\n",
    "        for index, row in combined_df.iterrows():\n",
    "            for aspect, sentiment in row['Sentiment_Results'].items():\n",
    "                if sentiment == \"Positive\":\n",
    "                    aspect_sentiments[aspect]['positive'] += 1\n",
    "                else:\n",
    "                    aspect_sentiments[aspect]['negative'] += 1\n",
    "\n",
    "        # Calculate sentiment percentages for each aspect\n",
    "        total_reviews = len(combined_df)\n",
    "        for aspect, counts in aspect_sentiments.items():\n",
    "            counts['positive_percent'] = (counts['positive'] / total_reviews) * 100\n",
    "            counts['negative_percent'] = (counts['negative'] / total_reviews) * 100\n",
    "\n",
    "        # Create a folder for the hotel in the output directory\n",
    "        hotel_output_dir = os.path.join(output_dir, hotel_dir)\n",
    "        if not os.path.exists(hotel_output_dir):\n",
    "            os.makedirs(hotel_output_dir)\n",
    "\n",
    "        # Save the sentiment analysis results to a CSV file\n",
    "        output_file_path = os.path.join(hotel_output_dir, f\"{hotel_dir}_sentiment_analysis.csv\")\n",
    "        output_df = pd.DataFrame(aspect_sentiments).T\n",
    "        output_df.to_csv(output_file_path)\n",
    "\n",
    "        print(f\"Processed {hotel_dir}, results saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the output directory where sentiment results are saved\n",
    "output_dir = r\"C:\\Users\\andyb\\Desktop\\Coding Files\\PointView\\datasets\\sentiment_results\"\n",
    "\n",
    "# Function to load and display sentiment results from all hotels\n",
    "def load_and_display_sentiment_results(output_dir):\n",
    "    for hotel_dir in os.listdir(output_dir):\n",
    "        hotel_path = os.path.join(output_dir, hotel_dir)\n",
    "        \n",
    "        if os.path.isdir(hotel_path):  # Check if it's a directory\n",
    "            for csv_file in os.listdir(hotel_path):\n",
    "                if csv_file.endswith('_sentiment_analysis.csv'):\n",
    "                    file_path = os.path.join(hotel_path, csv_file)\n",
    "                    \n",
    "                    # Load the CSV file into a DataFrame\n",
    "                    df = pd.read_csv(file_path, index_col=0)\n",
    "                    \n",
    "                    # Display the DataFrame\n",
    "                    print(f\"Sentiment Analysis for {hotel_dir}:\")\n",
    "                    print(df)\n",
    "                    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Call the function to display all sentiment results\n",
    "load_and_display_sentiment_results(output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KPI Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "\n",
    "# Define your specific KPIs\n",
    "specific_kpis = ['food', 'staff', 'comfort & facilities', 'value for money']\n",
    "\n",
    "# Function to extract keywords using TF-IDF\n",
    "def extract_keywords_tfidf(reviews, top_n=10):\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=10000, min_df=0.01, stop_words='english')\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(reviews)\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "    tfidf_scores = tfidf_matrix.toarray()\n",
    "    \n",
    "    top_keywords = []\n",
    "    for doc in tfidf_scores:\n",
    "        sorted_indices = doc.argsort()[-top_n:]\n",
    "        top_keywords.extend([feature_names[i] for i in sorted_indices])\n",
    "    \n",
    "    return Counter(top_keywords).most_common(top_n)\n",
    "\n",
    "# Example usage with a single hotel's reviews\n",
    "def get_additional_kpis(reviews):\n",
    "    keywords = extract_keywords_tfidf(reviews, top_n=10)\n",
    "    additional_kpis = [keyword for keyword, _ in keywords if keyword not in specific_kpis]\n",
    "    return additional_kpis\n",
    "\n",
    "# Directory paths\n",
    "base_dir = r\"C:\\Users\\andyb\\Desktop\\Coding Files\\PointView\\datasets\\hotel_aspect_based_dataset\"\n",
    "\n",
    "# Process each hotel's reviews\n",
    "for hotel_dir in os.listdir(base_dir):\n",
    "    hotel_path = os.path.join(base_dir, hotel_dir)\n",
    "    \n",
    "    if os.path.isdir(hotel_path):\n",
    "        combined_df = pd.DataFrame()\n",
    "\n",
    "        # Combine all CSV files within the hotel directory\n",
    "        for csv_file in os.listdir(hotel_path):\n",
    "            if csv_file.endswith('.csv'):\n",
    "                file_path = os.path.join(hotel_path, csv_file)\n",
    "                temp_df = pd.read_csv(file_path)\n",
    "                combined_df = pd.concat([combined_df, temp_df], ignore_index=True)\n",
    "\n",
    "        # Extract additional KPIs\n",
    "        reviews = combined_df['Review Content'].tolist()\n",
    "        additional_kpis = get_additional_kpis(reviews)\n",
    "        \n",
    "        print(f\"Additional KPIs identified for {hotel_dir}: {additional_kpis}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Function to preprocess reviews for LDA\n",
    "def preprocess_for_lda(reviews):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    texts = [[word for word in review.lower().split() if word not in stop_words] for review in reviews]\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    return corpus, dictionary\n",
    "\n",
    "# Function to extract topics using LDA\n",
    "def extract_topics_lda(corpus, dictionary, num_topics=5):\n",
    "    lda_model = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=15)\n",
    "    topics = lda_model.print_topics(num_words=5)\n",
    "    topic_keywords = []\n",
    "    for topic in topics:\n",
    "        words = topic[1].split(' + ')\n",
    "        keywords = [word.split('*')[-1].replace('\"', '').strip() for word in words]\n",
    "        topic_keywords.extend(keywords)\n",
    "    return list(set(topic_keywords))\n",
    "\n",
    "# Example usage with a single hotel's reviews\n",
    "def get_additional_kpis_lda(reviews):\n",
    "    corpus, dictionary = preprocess_for_lda(reviews)\n",
    "    topics = extract_topics_lda(corpus, dictionary, num_topics=5)\n",
    "    additional_kpis = [topic for topic in topics if topic not in specific_kpis]\n",
    "    return additional_kpis\n",
    "\n",
    "# Process each hotel's reviews\n",
    "for hotel_dir in os.listdir(base_dir):\n",
    "    hotel_path = os.path.join(base_dir, hotel_dir)\n",
    "    \n",
    "    if os.path.isdir(hotel_path):\n",
    "        combined_df = pd.DataFrame()\n",
    "\n",
    "        # Combine all CSV files within the hotel directory\n",
    "        for csv_file in os.listdir(hotel_path):\n",
    "            if csv_file.endswith('.csv'):\n",
    "                file_path = os.path.join(hotel_path, csv_file)\n",
    "                temp_df = pd.read_csv(file_path)\n",
    "                combined_df = pd.concat([combined_df, temp_df], ignore_index=True)\n",
    "\n",
    "        # Extract additional KPIs using LDA\n",
    "        reviews = combined_df['Review Content'].tolist()\n",
    "        additional_kpis_lda = get_additional_kpis_lda(reviews)\n",
    "        \n",
    "        print(f\"Additional KPIs identified using LDA for {hotel_dir}: {additional_kpis_lda}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_kpis(predefined_kpis, additional_kpis):\n",
    "    return list(set(predefined_kpis + additional_kpis))\n",
    "\n",
    "# Combine TF-IDF and LDA results with predefined KPIs\n",
    "for hotel_dir in os.listdir(base_dir):\n",
    "    hotel_path = os.path.join(base_dir, hotel_dir)\n",
    "    \n",
    "    if os.path.isdir(hotel_path):\n",
    "        combined_df = pd.DataFrame()\n",
    "\n",
    "        for csv_file in os.listdir(hotel_path):\n",
    "            if csv_file.endswith('.csv'):\n",
    "                file_path = os.path.join(hotel_path, csv_file)\n",
    "                temp_df = pd.read_csv(file_path)\n",
    "                combined_df = pd.concat([combined_df, temp_df], ignore_index=True)\n",
    "\n",
    "        reviews = combined_df['Review Content'].tolist()\n",
    "\n",
    "        # Extract additional KPIs using both TF-IDF and LDA\n",
    "        additional_kpis_tfidf = get_additional_kpis(reviews)\n",
    "        additional_kpis_lda = get_additional_kpis_lda(reviews)\n",
    "        additional_kpis = list(set(additional_kpis_tfidf + additional_kpis_lda))\n",
    "\n",
    "        # Combine with predefined KPIs\n",
    "        all_kpis = combine_kpis(specific_kpis, additional_kpis)\n",
    "\n",
    "        print(f\"All KPIs for {hotel_dir}: {all_kpis}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
